{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "001e0e29",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.applications import InceptionV3\n",
    "from tensorflow.keras.applications import VGG19\n",
    "from tensorflow.keras.applications import ResNet50\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Dropout,Input,Flatten,Dense,MaxPooling2D\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator  # Data Augumentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8928b1bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "batchsize=100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "bac365d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 61379 images belonging to 2 classes.\n",
      "Found 15343 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "train_datagen= ImageDataGenerator(rescale=1./255, rotation_range=0.2,shear_range=0.2,\n",
    "    zoom_range=0.2,width_shift_range=0.2,\n",
    "    height_shift_range=0.2, validation_split=0.2)\n",
    "\n",
    "train_data= train_datagen.flow_from_directory(r'C:\\Users\\vigna\\Desktop\\mini project\\prepared_data\\train',\n",
    "                                target_size=(80,80),batch_size=batchsize,class_mode='categorical',subset='training' )\n",
    "\n",
    "validation_data= train_datagen.flow_from_directory(r'C:\\Users\\vigna\\Desktop\\mini project\\prepared_data\\train',\n",
    "                                target_size=(80,80),batch_size=batchsize,class_mode='categorical', subset='validation')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "3ac2b684",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 8176 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "test_datagen = ImageDataGenerator(rescale=1./255)\n",
    "\n",
    "test_data = test_datagen.flow_from_directory(r'C:\\Users\\vigna\\Desktop\\mini project\\prepared_data\\test',\n",
    "                                target_size=(80,80),batch_size=batchsize,class_mode='categorical')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "7559becf",
   "metadata": {},
   "outputs": [],
   "source": [
    "bmodel = InceptionV3(include_top=False, weights='imagenet', input_tensor=Input(shape=(80,80,3)))\n",
    "hmodel = bmodel.output\n",
    "hmodel = Flatten()(hmodel)\n",
    "hmodel = Dense(64, activation='relu')(hmodel)\n",
    "hmodel = Dropout(0.5)(hmodel)\n",
    "hmodel = Dense(2,activation= 'softmax')(hmodel)\n",
    "\n",
    "model = Model(inputs=bmodel.input, outputs= hmodel)\n",
    "for layer in bmodel.layers:\n",
    "    layer.trainable = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "24c990b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"inception_v3\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_6 (InputLayer)           [(None, 80, 80, 3)]  0           []                               \n",
      "                                                                                                  \n",
      " conv2d_94 (Conv2D)             (None, 39, 39, 32)   864         ['input_6[0][0]']                \n",
      "                                                                                                  \n",
      " batch_normalization_94 (BatchN  (None, 39, 39, 32)  96          ['conv2d_94[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " activation_94 (Activation)     (None, 39, 39, 32)   0           ['batch_normalization_94[0][0]'] \n",
      "                                                                                                  \n",
      " conv2d_95 (Conv2D)             (None, 37, 37, 32)   9216        ['activation_94[0][0]']          \n",
      "                                                                                                  \n",
      " batch_normalization_95 (BatchN  (None, 37, 37, 32)  96          ['conv2d_95[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " activation_95 (Activation)     (None, 37, 37, 32)   0           ['batch_normalization_95[0][0]'] \n",
      "                                                                                                  \n",
      " conv2d_96 (Conv2D)             (None, 37, 37, 64)   18432       ['activation_95[0][0]']          \n",
      "                                                                                                  \n",
      " batch_normalization_96 (BatchN  (None, 37, 37, 64)  192         ['conv2d_96[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " activation_96 (Activation)     (None, 37, 37, 64)   0           ['batch_normalization_96[0][0]'] \n",
      "                                                                                                  \n",
      " max_pooling2d_4 (MaxPooling2D)  (None, 18, 18, 64)  0           ['activation_96[0][0]']          \n",
      "                                                                                                  \n",
      " conv2d_97 (Conv2D)             (None, 18, 18, 80)   5120        ['max_pooling2d_4[0][0]']        \n",
      "                                                                                                  \n",
      " batch_normalization_97 (BatchN  (None, 18, 18, 80)  240         ['conv2d_97[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " activation_97 (Activation)     (None, 18, 18, 80)   0           ['batch_normalization_97[0][0]'] \n",
      "                                                                                                  \n",
      " conv2d_98 (Conv2D)             (None, 16, 16, 192)  138240      ['activation_97[0][0]']          \n",
      "                                                                                                  \n",
      " batch_normalization_98 (BatchN  (None, 16, 16, 192)  576        ['conv2d_98[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " activation_98 (Activation)     (None, 16, 16, 192)  0           ['batch_normalization_98[0][0]'] \n",
      "                                                                                                  \n",
      " max_pooling2d_5 (MaxPooling2D)  (None, 7, 7, 192)   0           ['activation_98[0][0]']          \n",
      "                                                                                                  \n",
      " conv2d_102 (Conv2D)            (None, 7, 7, 64)     12288       ['max_pooling2d_5[0][0]']        \n",
      "                                                                                                  \n",
      " batch_normalization_102 (Batch  (None, 7, 7, 64)    192         ['conv2d_102[0][0]']             \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " activation_102 (Activation)    (None, 7, 7, 64)     0           ['batch_normalization_102[0][0]']\n",
      "                                                                                                  \n",
      " conv2d_100 (Conv2D)            (None, 7, 7, 48)     9216        ['max_pooling2d_5[0][0]']        \n",
      "                                                                                                  \n",
      " conv2d_103 (Conv2D)            (None, 7, 7, 96)     55296       ['activation_102[0][0]']         \n",
      "                                                                                                  \n",
      " batch_normalization_100 (Batch  (None, 7, 7, 48)    144         ['conv2d_100[0][0]']             \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " batch_normalization_103 (Batch  (None, 7, 7, 96)    288         ['conv2d_103[0][0]']             \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " activation_100 (Activation)    (None, 7, 7, 48)     0           ['batch_normalization_100[0][0]']\n",
      "                                                                                                  \n",
      " activation_103 (Activation)    (None, 7, 7, 96)     0           ['batch_normalization_103[0][0]']\n",
      "                                                                                                  \n",
      " average_pooling2d_9 (AveragePo  (None, 7, 7, 192)   0           ['max_pooling2d_5[0][0]']        \n",
      " oling2D)                                                                                         \n",
      "                                                                                                  \n",
      " conv2d_99 (Conv2D)             (None, 7, 7, 64)     12288       ['max_pooling2d_5[0][0]']        \n",
      "                                                                                                  \n",
      " conv2d_101 (Conv2D)            (None, 7, 7, 64)     76800       ['activation_100[0][0]']         \n",
      "                                                                                                  \n",
      " conv2d_104 (Conv2D)            (None, 7, 7, 96)     82944       ['activation_103[0][0]']         \n",
      "                                                                                                  \n",
      " conv2d_105 (Conv2D)            (None, 7, 7, 32)     6144        ['average_pooling2d_9[0][0]']    \n",
      "                                                                                                  \n",
      " batch_normalization_99 (BatchN  (None, 7, 7, 64)    192         ['conv2d_99[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " batch_normalization_101 (Batch  (None, 7, 7, 64)    192         ['conv2d_101[0][0]']             \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " batch_normalization_104 (Batch  (None, 7, 7, 96)    288         ['conv2d_104[0][0]']             \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " batch_normalization_105 (Batch  (None, 7, 7, 32)    96          ['conv2d_105[0][0]']             \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " activation_99 (Activation)     (None, 7, 7, 64)     0           ['batch_normalization_99[0][0]'] \n",
      "                                                                                                  \n",
      " activation_101 (Activation)    (None, 7, 7, 64)     0           ['batch_normalization_101[0][0]']\n",
      "                                                                                                  \n",
      " activation_104 (Activation)    (None, 7, 7, 96)     0           ['batch_normalization_104[0][0]']\n",
      "                                                                                                  \n",
      " activation_105 (Activation)    (None, 7, 7, 32)     0           ['batch_normalization_105[0][0]']\n",
      "                                                                                                  \n",
      " mixed0 (Concatenate)           (None, 7, 7, 256)    0           ['activation_99[0][0]',          \n",
      "                                                                  'activation_101[0][0]',         \n",
      "                                                                  'activation_104[0][0]',         \n",
      "                                                                  'activation_105[0][0]']         \n",
      "                                                                                                  \n",
      " conv2d_109 (Conv2D)            (None, 7, 7, 64)     16384       ['mixed0[0][0]']                 \n",
      "                                                                                                  \n",
      " batch_normalization_109 (Batch  (None, 7, 7, 64)    192         ['conv2d_109[0][0]']             \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " activation_109 (Activation)    (None, 7, 7, 64)     0           ['batch_normalization_109[0][0]']\n",
      "                                                                                                  \n",
      " conv2d_107 (Conv2D)            (None, 7, 7, 48)     12288       ['mixed0[0][0]']                 \n",
      "                                                                                                  \n",
      " conv2d_110 (Conv2D)            (None, 7, 7, 96)     55296       ['activation_109[0][0]']         \n",
      "                                                                                                  \n",
      " batch_normalization_107 (Batch  (None, 7, 7, 48)    144         ['conv2d_107[0][0]']             \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " batch_normalization_110 (Batch  (None, 7, 7, 96)    288         ['conv2d_110[0][0]']             \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " activation_107 (Activation)    (None, 7, 7, 48)     0           ['batch_normalization_107[0][0]']\n",
      "                                                                                                  \n",
      " activation_110 (Activation)    (None, 7, 7, 96)     0           ['batch_normalization_110[0][0]']\n",
      "                                                                                                  \n",
      " average_pooling2d_10 (AverageP  (None, 7, 7, 256)   0           ['mixed0[0][0]']                 \n",
      " ooling2D)                                                                                        \n",
      "                                                                                                  \n",
      " conv2d_106 (Conv2D)            (None, 7, 7, 64)     16384       ['mixed0[0][0]']                 \n",
      "                                                                                                  \n",
      " conv2d_108 (Conv2D)            (None, 7, 7, 64)     76800       ['activation_107[0][0]']         \n",
      "                                                                                                  \n",
      " conv2d_111 (Conv2D)            (None, 7, 7, 96)     82944       ['activation_110[0][0]']         \n",
      "                                                                                                  \n",
      " conv2d_112 (Conv2D)            (None, 7, 7, 64)     16384       ['average_pooling2d_10[0][0]']   \n",
      "                                                                                                  \n",
      " batch_normalization_106 (Batch  (None, 7, 7, 64)    192         ['conv2d_106[0][0]']             \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " batch_normalization_108 (Batch  (None, 7, 7, 64)    192         ['conv2d_108[0][0]']             \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " batch_normalization_111 (Batch  (None, 7, 7, 96)    288         ['conv2d_111[0][0]']             \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " batch_normalization_112 (Batch  (None, 7, 7, 64)    192         ['conv2d_112[0][0]']             \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " activation_106 (Activation)    (None, 7, 7, 64)     0           ['batch_normalization_106[0][0]']\n",
      "                                                                                                  \n",
      " activation_108 (Activation)    (None, 7, 7, 64)     0           ['batch_normalization_108[0][0]']\n",
      "                                                                                                  \n",
      " activation_111 (Activation)    (None, 7, 7, 96)     0           ['batch_normalization_111[0][0]']\n",
      "                                                                                                  \n",
      " activation_112 (Activation)    (None, 7, 7, 64)     0           ['batch_normalization_112[0][0]']\n",
      "                                                                                                  \n",
      " mixed1 (Concatenate)           (None, 7, 7, 288)    0           ['activation_106[0][0]',         \n",
      "                                                                  'activation_108[0][0]',         \n",
      "                                                                  'activation_111[0][0]',         \n",
      "                                                                  'activation_112[0][0]']         \n",
      "                                                                                                  \n",
      " conv2d_116 (Conv2D)            (None, 7, 7, 64)     18432       ['mixed1[0][0]']                 \n",
      "                                                                                                  \n",
      " batch_normalization_116 (Batch  (None, 7, 7, 64)    192         ['conv2d_116[0][0]']             \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " activation_116 (Activation)    (None, 7, 7, 64)     0           ['batch_normalization_116[0][0]']\n",
      "                                                                                                  \n",
      " conv2d_114 (Conv2D)            (None, 7, 7, 48)     13824       ['mixed1[0][0]']                 \n",
      "                                                                                                  \n",
      " conv2d_117 (Conv2D)            (None, 7, 7, 96)     55296       ['activation_116[0][0]']         \n",
      "                                                                                                  \n",
      " batch_normalization_114 (Batch  (None, 7, 7, 48)    144         ['conv2d_114[0][0]']             \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " batch_normalization_117 (Batch  (None, 7, 7, 96)    288         ['conv2d_117[0][0]']             \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " activation_114 (Activation)    (None, 7, 7, 48)     0           ['batch_normalization_114[0][0]']\n",
      "                                                                                                  \n",
      " activation_117 (Activation)    (None, 7, 7, 96)     0           ['batch_normalization_117[0][0]']\n",
      "                                                                                                  \n",
      " average_pooling2d_11 (AverageP  (None, 7, 7, 288)   0           ['mixed1[0][0]']                 \n",
      " ooling2D)                                                                                        \n",
      "                                                                                                  \n",
      " conv2d_113 (Conv2D)            (None, 7, 7, 64)     18432       ['mixed1[0][0]']                 \n",
      "                                                                                                  \n",
      " conv2d_115 (Conv2D)            (None, 7, 7, 64)     76800       ['activation_114[0][0]']         \n",
      "                                                                                                  \n",
      " conv2d_118 (Conv2D)            (None, 7, 7, 96)     82944       ['activation_117[0][0]']         \n",
      "                                                                                                  \n",
      " conv2d_119 (Conv2D)            (None, 7, 7, 64)     18432       ['average_pooling2d_11[0][0]']   \n",
      "                                                                                                  \n",
      " batch_normalization_113 (Batch  (None, 7, 7, 64)    192         ['conv2d_113[0][0]']             \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " batch_normalization_115 (Batch  (None, 7, 7, 64)    192         ['conv2d_115[0][0]']             \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " batch_normalization_118 (Batch  (None, 7, 7, 96)    288         ['conv2d_118[0][0]']             \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " batch_normalization_119 (Batch  (None, 7, 7, 64)    192         ['conv2d_119[0][0]']             \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " activation_113 (Activation)    (None, 7, 7, 64)     0           ['batch_normalization_113[0][0]']\n",
      "                                                                                                  \n",
      " activation_115 (Activation)    (None, 7, 7, 64)     0           ['batch_normalization_115[0][0]']\n",
      "                                                                                                  \n",
      " activation_118 (Activation)    (None, 7, 7, 96)     0           ['batch_normalization_118[0][0]']\n",
      "                                                                                                  \n",
      " activation_119 (Activation)    (None, 7, 7, 64)     0           ['batch_normalization_119[0][0]']\n",
      "                                                                                                  \n",
      " mixed2 (Concatenate)           (None, 7, 7, 288)    0           ['activation_113[0][0]',         \n",
      "                                                                  'activation_115[0][0]',         \n",
      "                                                                  'activation_118[0][0]',         \n",
      "                                                                  'activation_119[0][0]']         \n",
      "                                                                                                  \n",
      " conv2d_121 (Conv2D)            (None, 7, 7, 64)     18432       ['mixed2[0][0]']                 \n",
      "                                                                                                  \n",
      " batch_normalization_121 (Batch  (None, 7, 7, 64)    192         ['conv2d_121[0][0]']             \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " activation_121 (Activation)    (None, 7, 7, 64)     0           ['batch_normalization_121[0][0]']\n",
      "                                                                                                  \n",
      " conv2d_122 (Conv2D)            (None, 7, 7, 96)     55296       ['activation_121[0][0]']         \n",
      "                                                                                                  \n",
      " batch_normalization_122 (Batch  (None, 7, 7, 96)    288         ['conv2d_122[0][0]']             \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " activation_122 (Activation)    (None, 7, 7, 96)     0           ['batch_normalization_122[0][0]']\n",
      "                                                                                                  \n",
      " conv2d_120 (Conv2D)            (None, 3, 3, 384)    995328      ['mixed2[0][0]']                 \n",
      "                                                                                                  \n",
      " conv2d_123 (Conv2D)            (None, 3, 3, 96)     82944       ['activation_122[0][0]']         \n",
      "                                                                                                  \n",
      " batch_normalization_120 (Batch  (None, 3, 3, 384)   1152        ['conv2d_120[0][0]']             \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " batch_normalization_123 (Batch  (None, 3, 3, 96)    288         ['conv2d_123[0][0]']             \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " activation_120 (Activation)    (None, 3, 3, 384)    0           ['batch_normalization_120[0][0]']\n",
      "                                                                                                  \n",
      " activation_123 (Activation)    (None, 3, 3, 96)     0           ['batch_normalization_123[0][0]']\n",
      "                                                                                                  \n",
      " max_pooling2d_6 (MaxPooling2D)  (None, 3, 3, 288)   0           ['mixed2[0][0]']                 \n",
      "                                                                                                  \n",
      " mixed3 (Concatenate)           (None, 3, 3, 768)    0           ['activation_120[0][0]',         \n",
      "                                                                  'activation_123[0][0]',         \n",
      "                                                                  'max_pooling2d_6[0][0]']        \n",
      "                                                                                                  \n",
      " conv2d_128 (Conv2D)            (None, 3, 3, 128)    98304       ['mixed3[0][0]']                 \n",
      "                                                                                                  \n",
      " batch_normalization_128 (Batch  (None, 3, 3, 128)   384         ['conv2d_128[0][0]']             \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " activation_128 (Activation)    (None, 3, 3, 128)    0           ['batch_normalization_128[0][0]']\n",
      "                                                                                                  \n",
      " conv2d_129 (Conv2D)            (None, 3, 3, 128)    114688      ['activation_128[0][0]']         \n",
      "                                                                                                  \n",
      " batch_normalization_129 (Batch  (None, 3, 3, 128)   384         ['conv2d_129[0][0]']             \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " activation_129 (Activation)    (None, 3, 3, 128)    0           ['batch_normalization_129[0][0]']\n",
      "                                                                                                  \n",
      " conv2d_125 (Conv2D)            (None, 3, 3, 128)    98304       ['mixed3[0][0]']                 \n",
      "                                                                                                  \n",
      " conv2d_130 (Conv2D)            (None, 3, 3, 128)    114688      ['activation_129[0][0]']         \n",
      "                                                                                                  \n",
      " batch_normalization_125 (Batch  (None, 3, 3, 128)   384         ['conv2d_125[0][0]']             \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " batch_normalization_130 (Batch  (None, 3, 3, 128)   384         ['conv2d_130[0][0]']             \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " activation_125 (Activation)    (None, 3, 3, 128)    0           ['batch_normalization_125[0][0]']\n",
      "                                                                                                  \n",
      " activation_130 (Activation)    (None, 3, 3, 128)    0           ['batch_normalization_130[0][0]']\n",
      "                                                                                                  \n",
      " conv2d_126 (Conv2D)            (None, 3, 3, 128)    114688      ['activation_125[0][0]']         \n",
      "                                                                                                  \n",
      " conv2d_131 (Conv2D)            (None, 3, 3, 128)    114688      ['activation_130[0][0]']         \n",
      "                                                                                                  \n",
      " batch_normalization_126 (Batch  (None, 3, 3, 128)   384         ['conv2d_126[0][0]']             \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " batch_normalization_131 (Batch  (None, 3, 3, 128)   384         ['conv2d_131[0][0]']             \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " activation_126 (Activation)    (None, 3, 3, 128)    0           ['batch_normalization_126[0][0]']\n",
      "                                                                                                  \n",
      " activation_131 (Activation)    (None, 3, 3, 128)    0           ['batch_normalization_131[0][0]']\n",
      "                                                                                                  \n",
      " average_pooling2d_12 (AverageP  (None, 3, 3, 768)   0           ['mixed3[0][0]']                 \n",
      " ooling2D)                                                                                        \n",
      "                                                                                                  \n",
      " conv2d_124 (Conv2D)            (None, 3, 3, 192)    147456      ['mixed3[0][0]']                 \n",
      "                                                                                                  \n",
      " conv2d_127 (Conv2D)            (None, 3, 3, 192)    172032      ['activation_126[0][0]']         \n",
      "                                                                                                  \n",
      " conv2d_132 (Conv2D)            (None, 3, 3, 192)    172032      ['activation_131[0][0]']         \n",
      "                                                                                                  \n",
      " conv2d_133 (Conv2D)            (None, 3, 3, 192)    147456      ['average_pooling2d_12[0][0]']   \n",
      "                                                                                                  \n",
      " batch_normalization_124 (Batch  (None, 3, 3, 192)   576         ['conv2d_124[0][0]']             \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " batch_normalization_127 (Batch  (None, 3, 3, 192)   576         ['conv2d_127[0][0]']             \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " batch_normalization_132 (Batch  (None, 3, 3, 192)   576         ['conv2d_132[0][0]']             \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " batch_normalization_133 (Batch  (None, 3, 3, 192)   576         ['conv2d_133[0][0]']             \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " activation_124 (Activation)    (None, 3, 3, 192)    0           ['batch_normalization_124[0][0]']\n",
      "                                                                                                  \n",
      " activation_127 (Activation)    (None, 3, 3, 192)    0           ['batch_normalization_127[0][0]']\n",
      "                                                                                                  \n",
      " activation_132 (Activation)    (None, 3, 3, 192)    0           ['batch_normalization_132[0][0]']\n",
      "                                                                                                  \n",
      " activation_133 (Activation)    (None, 3, 3, 192)    0           ['batch_normalization_133[0][0]']\n",
      "                                                                                                  \n",
      " mixed4 (Concatenate)           (None, 3, 3, 768)    0           ['activation_124[0][0]',         \n",
      "                                                                  'activation_127[0][0]',         \n",
      "                                                                  'activation_132[0][0]',         \n",
      "                                                                  'activation_133[0][0]']         \n",
      "                                                                                                  \n",
      " conv2d_138 (Conv2D)            (None, 3, 3, 160)    122880      ['mixed4[0][0]']                 \n",
      "                                                                                                  \n",
      " batch_normalization_138 (Batch  (None, 3, 3, 160)   480         ['conv2d_138[0][0]']             \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " activation_138 (Activation)    (None, 3, 3, 160)    0           ['batch_normalization_138[0][0]']\n",
      "                                                                                                  \n",
      " conv2d_139 (Conv2D)            (None, 3, 3, 160)    179200      ['activation_138[0][0]']         \n",
      "                                                                                                  \n",
      " batch_normalization_139 (Batch  (None, 3, 3, 160)   480         ['conv2d_139[0][0]']             \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " activation_139 (Activation)    (None, 3, 3, 160)    0           ['batch_normalization_139[0][0]']\n",
      "                                                                                                  \n",
      " conv2d_135 (Conv2D)            (None, 3, 3, 160)    122880      ['mixed4[0][0]']                 \n",
      "                                                                                                  \n",
      " conv2d_140 (Conv2D)            (None, 3, 3, 160)    179200      ['activation_139[0][0]']         \n",
      "                                                                                                  \n",
      " batch_normalization_135 (Batch  (None, 3, 3, 160)   480         ['conv2d_135[0][0]']             \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " batch_normalization_140 (Batch  (None, 3, 3, 160)   480         ['conv2d_140[0][0]']             \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " activation_135 (Activation)    (None, 3, 3, 160)    0           ['batch_normalization_135[0][0]']\n",
      "                                                                                                  \n",
      " activation_140 (Activation)    (None, 3, 3, 160)    0           ['batch_normalization_140[0][0]']\n",
      "                                                                                                  \n",
      " conv2d_136 (Conv2D)            (None, 3, 3, 160)    179200      ['activation_135[0][0]']         \n",
      "                                                                                                  \n",
      " conv2d_141 (Conv2D)            (None, 3, 3, 160)    179200      ['activation_140[0][0]']         \n",
      "                                                                                                  \n",
      " batch_normalization_136 (Batch  (None, 3, 3, 160)   480         ['conv2d_136[0][0]']             \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " batch_normalization_141 (Batch  (None, 3, 3, 160)   480         ['conv2d_141[0][0]']             \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " activation_136 (Activation)    (None, 3, 3, 160)    0           ['batch_normalization_136[0][0]']\n",
      "                                                                                                  \n",
      " activation_141 (Activation)    (None, 3, 3, 160)    0           ['batch_normalization_141[0][0]']\n",
      "                                                                                                  \n",
      " average_pooling2d_13 (AverageP  (None, 3, 3, 768)   0           ['mixed4[0][0]']                 \n",
      " ooling2D)                                                                                        \n",
      "                                                                                                  \n",
      " conv2d_134 (Conv2D)            (None, 3, 3, 192)    147456      ['mixed4[0][0]']                 \n",
      "                                                                                                  \n",
      " conv2d_137 (Conv2D)            (None, 3, 3, 192)    215040      ['activation_136[0][0]']         \n",
      "                                                                                                  \n",
      " conv2d_142 (Conv2D)            (None, 3, 3, 192)    215040      ['activation_141[0][0]']         \n",
      "                                                                                                  \n",
      " conv2d_143 (Conv2D)            (None, 3, 3, 192)    147456      ['average_pooling2d_13[0][0]']   \n",
      "                                                                                                  \n",
      " batch_normalization_134 (Batch  (None, 3, 3, 192)   576         ['conv2d_134[0][0]']             \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " batch_normalization_137 (Batch  (None, 3, 3, 192)   576         ['conv2d_137[0][0]']             \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " batch_normalization_142 (Batch  (None, 3, 3, 192)   576         ['conv2d_142[0][0]']             \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " batch_normalization_143 (Batch  (None, 3, 3, 192)   576         ['conv2d_143[0][0]']             \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " activation_134 (Activation)    (None, 3, 3, 192)    0           ['batch_normalization_134[0][0]']\n",
      "                                                                                                  \n",
      " activation_137 (Activation)    (None, 3, 3, 192)    0           ['batch_normalization_137[0][0]']\n",
      "                                                                                                  \n",
      " activation_142 (Activation)    (None, 3, 3, 192)    0           ['batch_normalization_142[0][0]']\n",
      "                                                                                                  \n",
      " activation_143 (Activation)    (None, 3, 3, 192)    0           ['batch_normalization_143[0][0]']\n",
      "                                                                                                  \n",
      " mixed5 (Concatenate)           (None, 3, 3, 768)    0           ['activation_134[0][0]',         \n",
      "                                                                  'activation_137[0][0]',         \n",
      "                                                                  'activation_142[0][0]',         \n",
      "                                                                  'activation_143[0][0]']         \n",
      "                                                                                                  \n",
      " conv2d_148 (Conv2D)            (None, 3, 3, 160)    122880      ['mixed5[0][0]']                 \n",
      "                                                                                                  \n",
      " batch_normalization_148 (Batch  (None, 3, 3, 160)   480         ['conv2d_148[0][0]']             \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " activation_148 (Activation)    (None, 3, 3, 160)    0           ['batch_normalization_148[0][0]']\n",
      "                                                                                                  \n",
      " conv2d_149 (Conv2D)            (None, 3, 3, 160)    179200      ['activation_148[0][0]']         \n",
      "                                                                                                  \n",
      " batch_normalization_149 (Batch  (None, 3, 3, 160)   480         ['conv2d_149[0][0]']             \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " activation_149 (Activation)    (None, 3, 3, 160)    0           ['batch_normalization_149[0][0]']\n",
      "                                                                                                  \n",
      " conv2d_145 (Conv2D)            (None, 3, 3, 160)    122880      ['mixed5[0][0]']                 \n",
      "                                                                                                  \n",
      " conv2d_150 (Conv2D)            (None, 3, 3, 160)    179200      ['activation_149[0][0]']         \n",
      "                                                                                                  \n",
      " batch_normalization_145 (Batch  (None, 3, 3, 160)   480         ['conv2d_145[0][0]']             \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " batch_normalization_150 (Batch  (None, 3, 3, 160)   480         ['conv2d_150[0][0]']             \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " activation_145 (Activation)    (None, 3, 3, 160)    0           ['batch_normalization_145[0][0]']\n",
      "                                                                                                  \n",
      " activation_150 (Activation)    (None, 3, 3, 160)    0           ['batch_normalization_150[0][0]']\n",
      "                                                                                                  \n",
      " conv2d_146 (Conv2D)            (None, 3, 3, 160)    179200      ['activation_145[0][0]']         \n",
      "                                                                                                  \n",
      " conv2d_151 (Conv2D)            (None, 3, 3, 160)    179200      ['activation_150[0][0]']         \n",
      "                                                                                                  \n",
      " batch_normalization_146 (Batch  (None, 3, 3, 160)   480         ['conv2d_146[0][0]']             \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " batch_normalization_151 (Batch  (None, 3, 3, 160)   480         ['conv2d_151[0][0]']             \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " activation_146 (Activation)    (None, 3, 3, 160)    0           ['batch_normalization_146[0][0]']\n",
      "                                                                                                  \n",
      " activation_151 (Activation)    (None, 3, 3, 160)    0           ['batch_normalization_151[0][0]']\n",
      "                                                                                                  \n",
      " average_pooling2d_14 (AverageP  (None, 3, 3, 768)   0           ['mixed5[0][0]']                 \n",
      " ooling2D)                                                                                        \n",
      "                                                                                                  \n",
      " conv2d_144 (Conv2D)            (None, 3, 3, 192)    147456      ['mixed5[0][0]']                 \n",
      "                                                                                                  \n",
      " conv2d_147 (Conv2D)            (None, 3, 3, 192)    215040      ['activation_146[0][0]']         \n",
      "                                                                                                  \n",
      " conv2d_152 (Conv2D)            (None, 3, 3, 192)    215040      ['activation_151[0][0]']         \n",
      "                                                                                                  \n",
      " conv2d_153 (Conv2D)            (None, 3, 3, 192)    147456      ['average_pooling2d_14[0][0]']   \n",
      "                                                                                                  \n",
      " batch_normalization_144 (Batch  (None, 3, 3, 192)   576         ['conv2d_144[0][0]']             \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " batch_normalization_147 (Batch  (None, 3, 3, 192)   576         ['conv2d_147[0][0]']             \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " batch_normalization_152 (Batch  (None, 3, 3, 192)   576         ['conv2d_152[0][0]']             \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " batch_normalization_153 (Batch  (None, 3, 3, 192)   576         ['conv2d_153[0][0]']             \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " activation_144 (Activation)    (None, 3, 3, 192)    0           ['batch_normalization_144[0][0]']\n",
      "                                                                                                  \n",
      " activation_147 (Activation)    (None, 3, 3, 192)    0           ['batch_normalization_147[0][0]']\n",
      "                                                                                                  \n",
      " activation_152 (Activation)    (None, 3, 3, 192)    0           ['batch_normalization_152[0][0]']\n",
      "                                                                                                  \n",
      " activation_153 (Activation)    (None, 3, 3, 192)    0           ['batch_normalization_153[0][0]']\n",
      "                                                                                                  \n",
      " mixed6 (Concatenate)           (None, 3, 3, 768)    0           ['activation_144[0][0]',         \n",
      "                                                                  'activation_147[0][0]',         \n",
      "                                                                  'activation_152[0][0]',         \n",
      "                                                                  'activation_153[0][0]']         \n",
      "                                                                                                  \n",
      " conv2d_158 (Conv2D)            (None, 3, 3, 192)    147456      ['mixed6[0][0]']                 \n",
      "                                                                                                  \n",
      " batch_normalization_158 (Batch  (None, 3, 3, 192)   576         ['conv2d_158[0][0]']             \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " activation_158 (Activation)    (None, 3, 3, 192)    0           ['batch_normalization_158[0][0]']\n",
      "                                                                                                  \n",
      " conv2d_159 (Conv2D)            (None, 3, 3, 192)    258048      ['activation_158[0][0]']         \n",
      "                                                                                                  \n",
      " batch_normalization_159 (Batch  (None, 3, 3, 192)   576         ['conv2d_159[0][0]']             \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " activation_159 (Activation)    (None, 3, 3, 192)    0           ['batch_normalization_159[0][0]']\n",
      "                                                                                                  \n",
      " conv2d_155 (Conv2D)            (None, 3, 3, 192)    147456      ['mixed6[0][0]']                 \n",
      "                                                                                                  \n",
      " conv2d_160 (Conv2D)            (None, 3, 3, 192)    258048      ['activation_159[0][0]']         \n",
      "                                                                                                  \n",
      " batch_normalization_155 (Batch  (None, 3, 3, 192)   576         ['conv2d_155[0][0]']             \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " batch_normalization_160 (Batch  (None, 3, 3, 192)   576         ['conv2d_160[0][0]']             \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " activation_155 (Activation)    (None, 3, 3, 192)    0           ['batch_normalization_155[0][0]']\n",
      "                                                                                                  \n",
      " activation_160 (Activation)    (None, 3, 3, 192)    0           ['batch_normalization_160[0][0]']\n",
      "                                                                                                  \n",
      " conv2d_156 (Conv2D)            (None, 3, 3, 192)    258048      ['activation_155[0][0]']         \n",
      "                                                                                                  \n",
      " conv2d_161 (Conv2D)            (None, 3, 3, 192)    258048      ['activation_160[0][0]']         \n",
      "                                                                                                  \n",
      " batch_normalization_156 (Batch  (None, 3, 3, 192)   576         ['conv2d_156[0][0]']             \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " batch_normalization_161 (Batch  (None, 3, 3, 192)   576         ['conv2d_161[0][0]']             \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " activation_156 (Activation)    (None, 3, 3, 192)    0           ['batch_normalization_156[0][0]']\n",
      "                                                                                                  \n",
      " activation_161 (Activation)    (None, 3, 3, 192)    0           ['batch_normalization_161[0][0]']\n",
      "                                                                                                  \n",
      " average_pooling2d_15 (AverageP  (None, 3, 3, 768)   0           ['mixed6[0][0]']                 \n",
      " ooling2D)                                                                                        \n",
      "                                                                                                  \n",
      " conv2d_154 (Conv2D)            (None, 3, 3, 192)    147456      ['mixed6[0][0]']                 \n",
      "                                                                                                  \n",
      " conv2d_157 (Conv2D)            (None, 3, 3, 192)    258048      ['activation_156[0][0]']         \n",
      "                                                                                                  \n",
      " conv2d_162 (Conv2D)            (None, 3, 3, 192)    258048      ['activation_161[0][0]']         \n",
      "                                                                                                  \n",
      " conv2d_163 (Conv2D)            (None, 3, 3, 192)    147456      ['average_pooling2d_15[0][0]']   \n",
      "                                                                                                  \n",
      " batch_normalization_154 (Batch  (None, 3, 3, 192)   576         ['conv2d_154[0][0]']             \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " batch_normalization_157 (Batch  (None, 3, 3, 192)   576         ['conv2d_157[0][0]']             \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " batch_normalization_162 (Batch  (None, 3, 3, 192)   576         ['conv2d_162[0][0]']             \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " batch_normalization_163 (Batch  (None, 3, 3, 192)   576         ['conv2d_163[0][0]']             \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " activation_154 (Activation)    (None, 3, 3, 192)    0           ['batch_normalization_154[0][0]']\n",
      "                                                                                                  \n",
      " activation_157 (Activation)    (None, 3, 3, 192)    0           ['batch_normalization_157[0][0]']\n",
      "                                                                                                  \n",
      " activation_162 (Activation)    (None, 3, 3, 192)    0           ['batch_normalization_162[0][0]']\n",
      "                                                                                                  \n",
      " activation_163 (Activation)    (None, 3, 3, 192)    0           ['batch_normalization_163[0][0]']\n",
      "                                                                                                  \n",
      " mixed7 (Concatenate)           (None, 3, 3, 768)    0           ['activation_154[0][0]',         \n",
      "                                                                  'activation_157[0][0]',         \n",
      "                                                                  'activation_162[0][0]',         \n",
      "                                                                  'activation_163[0][0]']         \n",
      "                                                                                                  \n",
      " conv2d_166 (Conv2D)            (None, 3, 3, 192)    147456      ['mixed7[0][0]']                 \n",
      "                                                                                                  \n",
      " batch_normalization_166 (Batch  (None, 3, 3, 192)   576         ['conv2d_166[0][0]']             \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " activation_166 (Activation)    (None, 3, 3, 192)    0           ['batch_normalization_166[0][0]']\n",
      "                                                                                                  \n",
      " conv2d_167 (Conv2D)            (None, 3, 3, 192)    258048      ['activation_166[0][0]']         \n",
      "                                                                                                  \n",
      " batch_normalization_167 (Batch  (None, 3, 3, 192)   576         ['conv2d_167[0][0]']             \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " activation_167 (Activation)    (None, 3, 3, 192)    0           ['batch_normalization_167[0][0]']\n",
      "                                                                                                  \n",
      " conv2d_164 (Conv2D)            (None, 3, 3, 192)    147456      ['mixed7[0][0]']                 \n",
      "                                                                                                  \n",
      " conv2d_168 (Conv2D)            (None, 3, 3, 192)    258048      ['activation_167[0][0]']         \n",
      "                                                                                                  \n",
      " batch_normalization_164 (Batch  (None, 3, 3, 192)   576         ['conv2d_164[0][0]']             \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " batch_normalization_168 (Batch  (None, 3, 3, 192)   576         ['conv2d_168[0][0]']             \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " activation_164 (Activation)    (None, 3, 3, 192)    0           ['batch_normalization_164[0][0]']\n",
      "                                                                                                  \n",
      " activation_168 (Activation)    (None, 3, 3, 192)    0           ['batch_normalization_168[0][0]']\n",
      "                                                                                                  \n",
      " conv2d_165 (Conv2D)            (None, 1, 1, 320)    552960      ['activation_164[0][0]']         \n",
      "                                                                                                  \n",
      " conv2d_169 (Conv2D)            (None, 1, 1, 192)    331776      ['activation_168[0][0]']         \n",
      "                                                                                                  \n",
      " batch_normalization_165 (Batch  (None, 1, 1, 320)   960         ['conv2d_165[0][0]']             \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " batch_normalization_169 (Batch  (None, 1, 1, 192)   576         ['conv2d_169[0][0]']             \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " activation_165 (Activation)    (None, 1, 1, 320)    0           ['batch_normalization_165[0][0]']\n",
      "                                                                                                  \n",
      " activation_169 (Activation)    (None, 1, 1, 192)    0           ['batch_normalization_169[0][0]']\n",
      "                                                                                                  \n",
      " max_pooling2d_7 (MaxPooling2D)  (None, 1, 1, 768)   0           ['mixed7[0][0]']                 \n",
      "                                                                                                  \n",
      " mixed8 (Concatenate)           (None, 1, 1, 1280)   0           ['activation_165[0][0]',         \n",
      "                                                                  'activation_169[0][0]',         \n",
      "                                                                  'max_pooling2d_7[0][0]']        \n",
      "                                                                                                  \n",
      " conv2d_174 (Conv2D)            (None, 1, 1, 448)    573440      ['mixed8[0][0]']                 \n",
      "                                                                                                  \n",
      " batch_normalization_174 (Batch  (None, 1, 1, 448)   1344        ['conv2d_174[0][0]']             \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " activation_174 (Activation)    (None, 1, 1, 448)    0           ['batch_normalization_174[0][0]']\n",
      "                                                                                                  \n",
      " conv2d_171 (Conv2D)            (None, 1, 1, 384)    491520      ['mixed8[0][0]']                 \n",
      "                                                                                                  \n",
      " conv2d_175 (Conv2D)            (None, 1, 1, 384)    1548288     ['activation_174[0][0]']         \n",
      "                                                                                                  \n",
      " batch_normalization_171 (Batch  (None, 1, 1, 384)   1152        ['conv2d_171[0][0]']             \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " batch_normalization_175 (Batch  (None, 1, 1, 384)   1152        ['conv2d_175[0][0]']             \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " activation_171 (Activation)    (None, 1, 1, 384)    0           ['batch_normalization_171[0][0]']\n",
      "                                                                                                  \n",
      " activation_175 (Activation)    (None, 1, 1, 384)    0           ['batch_normalization_175[0][0]']\n",
      "                                                                                                  \n",
      " conv2d_172 (Conv2D)            (None, 1, 1, 384)    442368      ['activation_171[0][0]']         \n",
      "                                                                                                  \n",
      " conv2d_173 (Conv2D)            (None, 1, 1, 384)    442368      ['activation_171[0][0]']         \n",
      "                                                                                                  \n",
      " conv2d_176 (Conv2D)            (None, 1, 1, 384)    442368      ['activation_175[0][0]']         \n",
      "                                                                                                  \n",
      " conv2d_177 (Conv2D)            (None, 1, 1, 384)    442368      ['activation_175[0][0]']         \n",
      "                                                                                                  \n",
      " average_pooling2d_16 (AverageP  (None, 1, 1, 1280)  0           ['mixed8[0][0]']                 \n",
      " ooling2D)                                                                                        \n",
      "                                                                                                  \n",
      " conv2d_170 (Conv2D)            (None, 1, 1, 320)    409600      ['mixed8[0][0]']                 \n",
      "                                                                                                  \n",
      " batch_normalization_172 (Batch  (None, 1, 1, 384)   1152        ['conv2d_172[0][0]']             \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " batch_normalization_173 (Batch  (None, 1, 1, 384)   1152        ['conv2d_173[0][0]']             \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " batch_normalization_176 (Batch  (None, 1, 1, 384)   1152        ['conv2d_176[0][0]']             \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " batch_normalization_177 (Batch  (None, 1, 1, 384)   1152        ['conv2d_177[0][0]']             \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " conv2d_178 (Conv2D)            (None, 1, 1, 192)    245760      ['average_pooling2d_16[0][0]']   \n",
      "                                                                                                  \n",
      " batch_normalization_170 (Batch  (None, 1, 1, 320)   960         ['conv2d_170[0][0]']             \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " activation_172 (Activation)    (None, 1, 1, 384)    0           ['batch_normalization_172[0][0]']\n",
      "                                                                                                  \n",
      " activation_173 (Activation)    (None, 1, 1, 384)    0           ['batch_normalization_173[0][0]']\n",
      "                                                                                                  \n",
      " activation_176 (Activation)    (None, 1, 1, 384)    0           ['batch_normalization_176[0][0]']\n",
      "                                                                                                  \n",
      " activation_177 (Activation)    (None, 1, 1, 384)    0           ['batch_normalization_177[0][0]']\n",
      "                                                                                                  \n",
      " batch_normalization_178 (Batch  (None, 1, 1, 192)   576         ['conv2d_178[0][0]']             \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " activation_170 (Activation)    (None, 1, 1, 320)    0           ['batch_normalization_170[0][0]']\n",
      "                                                                                                  \n",
      " mixed9_0 (Concatenate)         (None, 1, 1, 768)    0           ['activation_172[0][0]',         \n",
      "                                                                  'activation_173[0][0]']         \n",
      "                                                                                                  \n",
      " concatenate_2 (Concatenate)    (None, 1, 1, 768)    0           ['activation_176[0][0]',         \n",
      "                                                                  'activation_177[0][0]']         \n",
      "                                                                                                  \n",
      " activation_178 (Activation)    (None, 1, 1, 192)    0           ['batch_normalization_178[0][0]']\n",
      "                                                                                                  \n",
      " mixed9 (Concatenate)           (None, 1, 1, 2048)   0           ['activation_170[0][0]',         \n",
      "                                                                  'mixed9_0[0][0]',               \n",
      "                                                                  'concatenate_2[0][0]',          \n",
      "                                                                  'activation_178[0][0]']         \n",
      "                                                                                                  \n",
      " conv2d_183 (Conv2D)            (None, 1, 1, 448)    917504      ['mixed9[0][0]']                 \n",
      "                                                                                                  \n",
      " batch_normalization_183 (Batch  (None, 1, 1, 448)   1344        ['conv2d_183[0][0]']             \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " activation_183 (Activation)    (None, 1, 1, 448)    0           ['batch_normalization_183[0][0]']\n",
      "                                                                                                  \n",
      " conv2d_180 (Conv2D)            (None, 1, 1, 384)    786432      ['mixed9[0][0]']                 \n",
      "                                                                                                  \n",
      " conv2d_184 (Conv2D)            (None, 1, 1, 384)    1548288     ['activation_183[0][0]']         \n",
      "                                                                                                  \n",
      " batch_normalization_180 (Batch  (None, 1, 1, 384)   1152        ['conv2d_180[0][0]']             \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " batch_normalization_184 (Batch  (None, 1, 1, 384)   1152        ['conv2d_184[0][0]']             \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " activation_180 (Activation)    (None, 1, 1, 384)    0           ['batch_normalization_180[0][0]']\n",
      "                                                                                                  \n",
      " activation_184 (Activation)    (None, 1, 1, 384)    0           ['batch_normalization_184[0][0]']\n",
      "                                                                                                  \n",
      " conv2d_181 (Conv2D)            (None, 1, 1, 384)    442368      ['activation_180[0][0]']         \n",
      "                                                                                                  \n",
      " conv2d_182 (Conv2D)            (None, 1, 1, 384)    442368      ['activation_180[0][0]']         \n",
      "                                                                                                  \n",
      " conv2d_185 (Conv2D)            (None, 1, 1, 384)    442368      ['activation_184[0][0]']         \n",
      "                                                                                                  \n",
      " conv2d_186 (Conv2D)            (None, 1, 1, 384)    442368      ['activation_184[0][0]']         \n",
      "                                                                                                  \n",
      " average_pooling2d_17 (AverageP  (None, 1, 1, 2048)  0           ['mixed9[0][0]']                 \n",
      " ooling2D)                                                                                        \n",
      "                                                                                                  \n",
      " conv2d_179 (Conv2D)            (None, 1, 1, 320)    655360      ['mixed9[0][0]']                 \n",
      "                                                                                                  \n",
      " batch_normalization_181 (Batch  (None, 1, 1, 384)   1152        ['conv2d_181[0][0]']             \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " batch_normalization_182 (Batch  (None, 1, 1, 384)   1152        ['conv2d_182[0][0]']             \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " batch_normalization_185 (Batch  (None, 1, 1, 384)   1152        ['conv2d_185[0][0]']             \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " batch_normalization_186 (Batch  (None, 1, 1, 384)   1152        ['conv2d_186[0][0]']             \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " conv2d_187 (Conv2D)            (None, 1, 1, 192)    393216      ['average_pooling2d_17[0][0]']   \n",
      "                                                                                                  \n",
      " batch_normalization_179 (Batch  (None, 1, 1, 320)   960         ['conv2d_179[0][0]']             \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " activation_181 (Activation)    (None, 1, 1, 384)    0           ['batch_normalization_181[0][0]']\n",
      "                                                                                                  \n",
      " activation_182 (Activation)    (None, 1, 1, 384)    0           ['batch_normalization_182[0][0]']\n",
      "                                                                                                  \n",
      " activation_185 (Activation)    (None, 1, 1, 384)    0           ['batch_normalization_185[0][0]']\n",
      "                                                                                                  \n",
      " activation_186 (Activation)    (None, 1, 1, 384)    0           ['batch_normalization_186[0][0]']\n",
      "                                                                                                  \n",
      " batch_normalization_187 (Batch  (None, 1, 1, 192)   576         ['conv2d_187[0][0]']             \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " activation_179 (Activation)    (None, 1, 1, 320)    0           ['batch_normalization_179[0][0]']\n",
      "                                                                                                  \n",
      " mixed9_1 (Concatenate)         (None, 1, 1, 768)    0           ['activation_181[0][0]',         \n",
      "                                                                  'activation_182[0][0]']         \n",
      "                                                                                                  \n",
      " concatenate_3 (Concatenate)    (None, 1, 1, 768)    0           ['activation_185[0][0]',         \n",
      "                                                                  'activation_186[0][0]']         \n",
      "                                                                                                  \n",
      " activation_187 (Activation)    (None, 1, 1, 192)    0           ['batch_normalization_187[0][0]']\n",
      "                                                                                                  \n",
      " mixed10 (Concatenate)          (None, 1, 1, 2048)   0           ['activation_179[0][0]',         \n",
      "                                                                  'mixed9_1[0][0]',               \n",
      "                                                                  'concatenate_3[0][0]',          \n",
      "                                                                  'activation_187[0][0]']         \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 21,802,784\n",
      "Trainable params: 0\n",
      "Non-trainable params: 21,802,784\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "bmodel.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "4639623a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.callbacks import ModelCheckpoint,EarlyStopping, ReduceLROnPlateau"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "d5218c6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "checkpoint = ModelCheckpoint(r'C:\\Users\\vigna\\Desktop\\mini project\\model\\model_ensemble.hdf5',\n",
    "                            monitor='val_loss',save_best_only=True,verbose=3)\n",
    "\n",
    "learning_rate = ReduceLROnPlateau(monitor= 'val_loss', patience=3, verbose= 3 )\n",
    "\n",
    "callbacks=[checkpoint,learning_rate]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "9dff0383",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "614/614 [==============================] - ETA: 0s - loss: 0.2114 - accuracy: 0.9161\n",
      "Epoch 1: val_loss improved from inf to 0.19900, saving model to C:\\Users\\vigna\\Desktop\\mini project\\model\\model_ensemble.hdf5\n",
      "614/614 [==============================] - 215s 345ms/step - loss: 0.2114 - accuracy: 0.9161 - val_loss: 0.1990 - val_accuracy: 0.9138 - lr: 0.0010\n",
      "Epoch 2/50\n",
      "614/614 [==============================] - ETA: 0s - loss: 0.1700 - accuracy: 0.9349\n",
      "Epoch 2: val_loss did not improve from 0.19900\n",
      "614/614 [==============================] - 178s 290ms/step - loss: 0.1700 - accuracy: 0.9349 - val_loss: 0.2064 - val_accuracy: 0.9092 - lr: 0.0010\n",
      "Epoch 3/50\n",
      "614/614 [==============================] - ETA: 0s - loss: 0.1619 - accuracy: 0.9382\n",
      "Epoch 3: val_loss improved from 0.19900 to 0.19308, saving model to C:\\Users\\vigna\\Desktop\\mini project\\model\\model_ensemble.hdf5\n",
      "614/614 [==============================] - 180s 293ms/step - loss: 0.1619 - accuracy: 0.9382 - val_loss: 0.1931 - val_accuracy: 0.9168 - lr: 0.0010\n",
      "Epoch 4/50\n",
      "614/614 [==============================] - ETA: 0s - loss: 0.1566 - accuracy: 0.9395\n",
      "Epoch 4: val_loss did not improve from 0.19308\n",
      "614/614 [==============================] - 177s 289ms/step - loss: 0.1566 - accuracy: 0.9395 - val_loss: 0.1979 - val_accuracy: 0.9177 - lr: 0.0010\n",
      "Epoch 5/50\n",
      "614/614 [==============================] - ETA: 0s - loss: 0.1500 - accuracy: 0.9436\n",
      "Epoch 5: val_loss did not improve from 0.19308\n",
      "614/614 [==============================] - 177s 288ms/step - loss: 0.1500 - accuracy: 0.9436 - val_loss: 0.2018 - val_accuracy: 0.9183 - lr: 0.0010\n",
      "Epoch 6/50\n",
      "614/614 [==============================] - ETA: 0s - loss: 0.1479 - accuracy: 0.9428\n",
      "Epoch 6: val_loss did not improve from 0.19308\n",
      "\n",
      "Epoch 6: ReduceLROnPlateau reducing learning rate to 0.00010000000474974513.\n",
      "614/614 [==============================] - 179s 292ms/step - loss: 0.1479 - accuracy: 0.9428 - val_loss: 0.2001 - val_accuracy: 0.9239 - lr: 0.0010\n",
      "Epoch 7/50\n",
      "614/614 [==============================] - ETA: 0s - loss: 0.1412 - accuracy: 0.9461\n",
      "Epoch 7: val_loss improved from 0.19308 to 0.18335, saving model to C:\\Users\\vigna\\Desktop\\mini project\\model\\model_ensemble.hdf5\n",
      "614/614 [==============================] - 175s 284ms/step - loss: 0.1412 - accuracy: 0.9461 - val_loss: 0.1833 - val_accuracy: 0.9271 - lr: 1.0000e-04\n",
      "Epoch 8/50\n",
      "614/614 [==============================] - ETA: 0s - loss: 0.1366 - accuracy: 0.9482\n",
      "Epoch 8: val_loss did not improve from 0.18335\n",
      "614/614 [==============================] - 172s 280ms/step - loss: 0.1366 - accuracy: 0.9482 - val_loss: 0.1879 - val_accuracy: 0.9264 - lr: 1.0000e-04\n",
      "Epoch 9/50\n",
      "614/614 [==============================] - ETA: 0s - loss: 0.1366 - accuracy: 0.9489\n",
      "Epoch 9: val_loss did not improve from 0.18335\n",
      "614/614 [==============================] - 171s 279ms/step - loss: 0.1366 - accuracy: 0.9489 - val_loss: 0.1888 - val_accuracy: 0.9258 - lr: 1.0000e-04\n",
      "Epoch 10/50\n",
      "614/614 [==============================] - ETA: 0s - loss: 0.1332 - accuracy: 0.9501\n",
      "Epoch 10: val_loss did not improve from 0.18335\n",
      "\n",
      "Epoch 10: ReduceLROnPlateau reducing learning rate to 1.0000000474974514e-05.\n",
      "614/614 [==============================] - 173s 281ms/step - loss: 0.1332 - accuracy: 0.9501 - val_loss: 0.1866 - val_accuracy: 0.9255 - lr: 1.0000e-04\n",
      "Epoch 11/50\n",
      "614/614 [==============================] - ETA: 0s - loss: 0.1337 - accuracy: 0.9487\n",
      "Epoch 11: val_loss did not improve from 0.18335\n",
      "614/614 [==============================] - 173s 282ms/step - loss: 0.1337 - accuracy: 0.9487 - val_loss: 0.1890 - val_accuracy: 0.9260 - lr: 1.0000e-05\n",
      "Epoch 12/50\n",
      "614/614 [==============================] - ETA: 0s - loss: 0.1344 - accuracy: 0.9484\n",
      "Epoch 12: val_loss did not improve from 0.18335\n",
      "614/614 [==============================] - 177s 288ms/step - loss: 0.1344 - accuracy: 0.9484 - val_loss: 0.1888 - val_accuracy: 0.9244 - lr: 1.0000e-05\n",
      "Epoch 13/50\n",
      "614/614 [==============================] - ETA: 0s - loss: 0.1336 - accuracy: 0.9498\n",
      "Epoch 13: val_loss did not improve from 0.18335\n",
      "\n",
      "Epoch 13: ReduceLROnPlateau reducing learning rate to 1.0000000656873453e-06.\n",
      "614/614 [==============================] - 180s 294ms/step - loss: 0.1336 - accuracy: 0.9498 - val_loss: 0.1907 - val_accuracy: 0.9257 - lr: 1.0000e-05\n",
      "Epoch 14/50\n",
      "614/614 [==============================] - ETA: 0s - loss: 0.1337 - accuracy: 0.9501\n",
      "Epoch 14: val_loss did not improve from 0.18335\n",
      "614/614 [==============================] - 171s 279ms/step - loss: 0.1337 - accuracy: 0.9501 - val_loss: 0.1883 - val_accuracy: 0.9260 - lr: 1.0000e-06\n",
      "Epoch 15/50\n",
      "614/614 [==============================] - ETA: 0s - loss: 0.1351 - accuracy: 0.9486\n",
      "Epoch 15: val_loss did not improve from 0.18335\n",
      "614/614 [==============================] - 173s 281ms/step - loss: 0.1351 - accuracy: 0.9486 - val_loss: 0.1870 - val_accuracy: 0.9247 - lr: 1.0000e-06\n",
      "Epoch 16/50\n",
      "614/614 [==============================] - ETA: 0s - loss: 0.1351 - accuracy: 0.9488\n",
      "Epoch 16: val_loss did not improve from 0.18335\n",
      "\n",
      "Epoch 16: ReduceLROnPlateau reducing learning rate to 1.0000001111620805e-07.\n",
      "614/614 [==============================] - 172s 279ms/step - loss: 0.1351 - accuracy: 0.9488 - val_loss: 0.1837 - val_accuracy: 0.9264 - lr: 1.0000e-06\n",
      "Epoch 17/50\n",
      "614/614 [==============================] - ETA: 0s - loss: 0.1361 - accuracy: 0.9484\n",
      "Epoch 17: val_loss improved from 0.18335 to 0.18179, saving model to C:\\Users\\vigna\\Desktop\\mini project\\model\\model_ensemble.hdf5\n",
      "614/614 [==============================] - 173s 281ms/step - loss: 0.1361 - accuracy: 0.9484 - val_loss: 0.1818 - val_accuracy: 0.9292 - lr: 1.0000e-07\n",
      "Epoch 18/50\n",
      "614/614 [==============================] - ETA: 0s - loss: 0.1330 - accuracy: 0.9499\n",
      "Epoch 18: val_loss did not improve from 0.18179\n",
      "614/614 [==============================] - 173s 283ms/step - loss: 0.1330 - accuracy: 0.9499 - val_loss: 0.1868 - val_accuracy: 0.9280 - lr: 1.0000e-07\n",
      "Epoch 19/50\n",
      "614/614 [==============================] - ETA: 0s - loss: 0.1346 - accuracy: 0.9492\n",
      "Epoch 19: val_loss did not improve from 0.18179\n",
      "614/614 [==============================] - 173s 281ms/step - loss: 0.1346 - accuracy: 0.9492 - val_loss: 0.1884 - val_accuracy: 0.9278 - lr: 1.0000e-07\n",
      "Epoch 20/50\n",
      "614/614 [==============================] - ETA: 0s - loss: 0.1333 - accuracy: 0.9500\n",
      "Epoch 20: val_loss did not improve from 0.18179\n",
      "\n",
      "Epoch 20: ReduceLROnPlateau reducing learning rate to 1.000000082740371e-08.\n",
      "614/614 [==============================] - 173s 282ms/step - loss: 0.1333 - accuracy: 0.9500 - val_loss: 0.1912 - val_accuracy: 0.9271 - lr: 1.0000e-07\n",
      "Epoch 21/50\n",
      "614/614 [==============================] - ETA: 0s - loss: 0.1319 - accuracy: 0.9501\n",
      "Epoch 21: val_loss did not improve from 0.18179\n",
      "614/614 [==============================] - 174s 283ms/step - loss: 0.1319 - accuracy: 0.9501 - val_loss: 0.1862 - val_accuracy: 0.9254 - lr: 1.0000e-08\n",
      "Epoch 22/50\n",
      "614/614 [==============================] - ETA: 0s - loss: 0.1348 - accuracy: 0.9487\n",
      "Epoch 22: val_loss did not improve from 0.18179\n",
      "614/614 [==============================] - 174s 283ms/step - loss: 0.1348 - accuracy: 0.9487 - val_loss: 0.1866 - val_accuracy: 0.9271 - lr: 1.0000e-08\n",
      "Epoch 23/50\n",
      "614/614 [==============================] - ETA: 0s - loss: 0.1344 - accuracy: 0.9484\n",
      "Epoch 23: val_loss did not improve from 0.18179\n",
      "\n",
      "Epoch 23: ReduceLROnPlateau reducing learning rate to 1.000000082740371e-09.\n",
      "614/614 [==============================] - 172s 280ms/step - loss: 0.1344 - accuracy: 0.9484 - val_loss: 0.1863 - val_accuracy: 0.9286 - lr: 1.0000e-08\n",
      "Epoch 24/50\n",
      "614/614 [==============================] - ETA: 0s - loss: 0.1334 - accuracy: 0.9488\n",
      "Epoch 24: val_loss did not improve from 0.18179\n",
      "614/614 [==============================] - 173s 282ms/step - loss: 0.1334 - accuracy: 0.9488 - val_loss: 0.1941 - val_accuracy: 0.9237 - lr: 1.0000e-09\n",
      "Epoch 25/50\n",
      "614/614 [==============================] - ETA: 0s - loss: 0.1338 - accuracy: 0.9506\n",
      "Epoch 25: val_loss did not improve from 0.18179\n",
      "614/614 [==============================] - 173s 281ms/step - loss: 0.1338 - accuracy: 0.9506 - val_loss: 0.1839 - val_accuracy: 0.9286 - lr: 1.0000e-09\n",
      "Epoch 26/50\n",
      "614/614 [==============================] - ETA: 0s - loss: 0.1337 - accuracy: 0.9494\n",
      "Epoch 26: val_loss did not improve from 0.18179\n",
      "\n",
      "Epoch 26: ReduceLROnPlateau reducing learning rate to 1.000000082740371e-10.\n",
      "614/614 [==============================] - 172s 280ms/step - loss: 0.1337 - accuracy: 0.9494 - val_loss: 0.1888 - val_accuracy: 0.9271 - lr: 1.0000e-09\n",
      "Epoch 27/50\n",
      "614/614 [==============================] - ETA: 0s - loss: 0.1342 - accuracy: 0.9490\n",
      "Epoch 27: val_loss did not improve from 0.18179\n",
      "614/614 [==============================] - 173s 282ms/step - loss: 0.1342 - accuracy: 0.9490 - val_loss: 0.1895 - val_accuracy: 0.9259 - lr: 1.0000e-10\n",
      "Epoch 28/50\n",
      "614/614 [==============================] - ETA: 0s - loss: 0.1355 - accuracy: 0.9493\n",
      "Epoch 28: val_loss did not improve from 0.18179\n",
      "614/614 [==============================] - 172s 281ms/step - loss: 0.1355 - accuracy: 0.9493 - val_loss: 0.1880 - val_accuracy: 0.9244 - lr: 1.0000e-10\n",
      "Epoch 29/50\n",
      "614/614 [==============================] - ETA: 0s - loss: 0.1360 - accuracy: 0.9483\n",
      "Epoch 29: val_loss did not improve from 0.18179\n",
      "\n",
      "Epoch 29: ReduceLROnPlateau reducing learning rate to 1.000000082740371e-11.\n",
      "614/614 [==============================] - 172s 281ms/step - loss: 0.1360 - accuracy: 0.9483 - val_loss: 0.1839 - val_accuracy: 0.9265 - lr: 1.0000e-10\n",
      "Epoch 30/50\n",
      "614/614 [==============================] - ETA: 0s - loss: 0.1331 - accuracy: 0.9488\n",
      "Epoch 30: val_loss did not improve from 0.18179\n",
      "614/614 [==============================] - 171s 279ms/step - loss: 0.1331 - accuracy: 0.9488 - val_loss: 0.1941 - val_accuracy: 0.9230 - lr: 1.0000e-11\n",
      "Epoch 31/50\n",
      "614/614 [==============================] - ETA: 0s - loss: 0.1358 - accuracy: 0.9487\n",
      "Epoch 31: val_loss did not improve from 0.18179\n",
      "614/614 [==============================] - 170s 277ms/step - loss: 0.1358 - accuracy: 0.9487 - val_loss: 0.1960 - val_accuracy: 0.9239 - lr: 1.0000e-11\n",
      "Epoch 32/50\n",
      "614/614 [==============================] - ETA: 0s - loss: 0.1355 - accuracy: 0.9487\n",
      "Epoch 32: val_loss did not improve from 0.18179\n",
      "\n",
      "Epoch 32: ReduceLROnPlateau reducing learning rate to 1.000000082740371e-12.\n",
      "614/614 [==============================] - 169s 276ms/step - loss: 0.1355 - accuracy: 0.9487 - val_loss: 0.1879 - val_accuracy: 0.9276 - lr: 1.0000e-11\n",
      "Epoch 33/50\n",
      "614/614 [==============================] - ETA: 0s - loss: 0.1334 - accuracy: 0.9491\n",
      "Epoch 33: val_loss did not improve from 0.18179\n",
      "614/614 [==============================] - 169s 275ms/step - loss: 0.1334 - accuracy: 0.9491 - val_loss: 0.1854 - val_accuracy: 0.9269 - lr: 1.0000e-12\n",
      "Epoch 34/50\n",
      "614/614 [==============================] - ETA: 0s - loss: 0.1360 - accuracy: 0.9485\n",
      "Epoch 34: val_loss did not improve from 0.18179\n",
      "614/614 [==============================] - 169s 275ms/step - loss: 0.1360 - accuracy: 0.9485 - val_loss: 0.1868 - val_accuracy: 0.9262 - lr: 1.0000e-12\n",
      "Epoch 35/50\n",
      "614/614 [==============================] - ETA: 0s - loss: 0.1344 - accuracy: 0.9487\n",
      "Epoch 35: val_loss did not improve from 0.18179\n",
      "\n",
      "Epoch 35: ReduceLROnPlateau reducing learning rate to 1.0000001044244145e-13.\n",
      "614/614 [==============================] - 171s 278ms/step - loss: 0.1344 - accuracy: 0.9487 - val_loss: 0.1895 - val_accuracy: 0.9244 - lr: 1.0000e-12\n",
      "Epoch 36/50\n",
      "614/614 [==============================] - ETA: 0s - loss: 0.1344 - accuracy: 0.9496\n",
      "Epoch 36: val_loss did not improve from 0.18179\n",
      "614/614 [==============================] - 169s 274ms/step - loss: 0.1344 - accuracy: 0.9496 - val_loss: 0.1923 - val_accuracy: 0.9258 - lr: 1.0000e-13\n",
      "Epoch 37/50\n",
      "614/614 [==============================] - ETA: 0s - loss: 0.1335 - accuracy: 0.9488\n",
      "Epoch 37: val_loss did not improve from 0.18179\n",
      "614/614 [==============================] - 168s 273ms/step - loss: 0.1335 - accuracy: 0.9488 - val_loss: 0.1908 - val_accuracy: 0.9256 - lr: 1.0000e-13\n",
      "Epoch 38/50\n",
      "614/614 [==============================] - ETA: 0s - loss: 0.1343 - accuracy: 0.9497\n",
      "Epoch 38: val_loss did not improve from 0.18179\n",
      "\n",
      "Epoch 38: ReduceLROnPlateau reducing learning rate to 1.0000001179769417e-14.\n",
      "614/614 [==============================] - 173s 282ms/step - loss: 0.1343 - accuracy: 0.9497 - val_loss: 0.1908 - val_accuracy: 0.9255 - lr: 1.0000e-13\n",
      "Epoch 39/50\n",
      "614/614 [==============================] - ETA: 0s - loss: 0.1351 - accuracy: 0.9485\n",
      "Epoch 39: val_loss did not improve from 0.18179\n",
      "614/614 [==============================] - 173s 282ms/step - loss: 0.1351 - accuracy: 0.9485 - val_loss: 0.1889 - val_accuracy: 0.9263 - lr: 1.0000e-14\n",
      "Epoch 40/50\n",
      "614/614 [==============================] - ETA: 0s - loss: 0.1331 - accuracy: 0.9501\n",
      "Epoch 40: val_loss did not improve from 0.18179\n",
      "614/614 [==============================] - 180s 293ms/step - loss: 0.1331 - accuracy: 0.9501 - val_loss: 0.1857 - val_accuracy: 0.9278 - lr: 1.0000e-14\n",
      "Epoch 41/50\n",
      "614/614 [==============================] - ETA: 0s - loss: 0.1340 - accuracy: 0.9481\n",
      "Epoch 41: val_loss did not improve from 0.18179\n",
      "\n",
      "Epoch 41: ReduceLROnPlateau reducing learning rate to 1.0000001518582595e-15.\n",
      "614/614 [==============================] - 175s 284ms/step - loss: 0.1340 - accuracy: 0.9481 - val_loss: 0.1902 - val_accuracy: 0.9248 - lr: 1.0000e-14\n",
      "Epoch 42/50\n",
      "614/614 [==============================] - ETA: 0s - loss: 0.1362 - accuracy: 0.9482\n",
      "Epoch 42: val_loss did not improve from 0.18179\n",
      "614/614 [==============================] - 170s 277ms/step - loss: 0.1362 - accuracy: 0.9482 - val_loss: 0.1839 - val_accuracy: 0.9269 - lr: 1.0000e-15\n",
      "Epoch 43/50\n",
      "614/614 [==============================] - ETA: 0s - loss: 0.1344 - accuracy: 0.9488\n",
      "Epoch 43: val_loss did not improve from 0.18179\n",
      "614/614 [==============================] - 166s 271ms/step - loss: 0.1344 - accuracy: 0.9488 - val_loss: 0.1882 - val_accuracy: 0.9250 - lr: 1.0000e-15\n",
      "Epoch 44/50\n",
      "614/614 [==============================] - ETA: 0s - loss: 0.1356 - accuracy: 0.9490\n",
      "Epoch 44: val_loss did not improve from 0.18179\n",
      "\n",
      "Epoch 44: ReduceLROnPlateau reducing learning rate to 1.0000001095066122e-16.\n",
      "614/614 [==============================] - 167s 272ms/step - loss: 0.1356 - accuracy: 0.9490 - val_loss: 0.1864 - val_accuracy: 0.9264 - lr: 1.0000e-15\n",
      "Epoch 45/50\n",
      "614/614 [==============================] - ETA: 0s - loss: 0.1354 - accuracy: 0.9479\n",
      "Epoch 45: val_loss did not improve from 0.18179\n",
      "614/614 [==============================] - 166s 271ms/step - loss: 0.1354 - accuracy: 0.9479 - val_loss: 0.1928 - val_accuracy: 0.9243 - lr: 1.0000e-16\n",
      "Epoch 46/50\n",
      "614/614 [==============================] - ETA: 0s - loss: 0.1301 - accuracy: 0.9504\n",
      "Epoch 46: val_loss did not improve from 0.18179\n",
      "614/614 [==============================] - 165s 269ms/step - loss: 0.1301 - accuracy: 0.9504 - val_loss: 0.1855 - val_accuracy: 0.9260 - lr: 1.0000e-16\n",
      "Epoch 47/50\n",
      "614/614 [==============================] - ETA: 0s - loss: 0.1359 - accuracy: 0.9482\n",
      "Epoch 47: val_loss did not improve from 0.18179\n",
      "\n",
      "Epoch 47: ReduceLROnPlateau reducing learning rate to 1.0000000830368326e-17.\n",
      "614/614 [==============================] - 165s 269ms/step - loss: 0.1359 - accuracy: 0.9482 - val_loss: 0.1867 - val_accuracy: 0.9262 - lr: 1.0000e-16\n",
      "Epoch 48/50\n",
      "614/614 [==============================] - ETA: 0s - loss: 0.1345 - accuracy: 0.9487\n",
      "Epoch 48: val_loss did not improve from 0.18179\n",
      "614/614 [==============================] - 165s 269ms/step - loss: 0.1345 - accuracy: 0.9487 - val_loss: 0.1845 - val_accuracy: 0.9283 - lr: 1.0000e-17\n",
      "Epoch 49/50\n",
      "614/614 [==============================] - ETA: 0s - loss: 0.1361 - accuracy: 0.9491\n",
      "Epoch 49: val_loss did not improve from 0.18179\n",
      "614/614 [==============================] - 165s 269ms/step - loss: 0.1361 - accuracy: 0.9491 - val_loss: 0.1866 - val_accuracy: 0.9271 - lr: 1.0000e-17\n",
      "Epoch 50/50\n",
      "614/614 [==============================] - ETA: 0s - loss: 0.1346 - accuracy: 0.9485\n",
      "Epoch 50: val_loss did not improve from 0.18179\n",
      "\n",
      "Epoch 50: ReduceLROnPlateau reducing learning rate to 1.0000000664932204e-18.\n",
      "614/614 [==============================] - 165s 269ms/step - loss: 0.1346 - accuracy: 0.9485 - val_loss: 0.1900 - val_accuracy: 0.9269 - lr: 1.0000e-17\n"
     ]
    }
   ],
   "source": [
    "model.compile(optimizer='Adam', loss='categorical_crossentropy',metrics=['accuracy'])\n",
    "history1=model.fit(train_data,\n",
    "                   validation_data=validation_data,\n",
    "                   callbacks=callbacks,\n",
    "                    epochs=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "9237de27",
   "metadata": {},
   "outputs": [],
   "source": [
    "bmodel1 = VGG19(include_top=False, weights='imagenet', input_tensor=Input(shape=(80,80,3)))\n",
    "hmodel1 = bmodel1.output\n",
    "hmodel1 = Flatten()(hmodel1)\n",
    "hmodel1 = Dense(64, activation='relu')(hmodel1)\n",
    "hmodel1 = Dropout(0.5)(hmodel1)\n",
    "hmodel1 = Dense(2,activation= 'softmax')(hmodel1)\n",
    "\n",
    "model1 = Model(inputs=bmodel1.input, outputs= hmodel1)\n",
    "for layer in bmodel1.layers:\n",
    "    layer.trainable = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "70dd9bd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint1 = ModelCheckpoint(r'C:\\Users\\vigna\\Desktop\\mini project\\model\\model_vgg19_ensemble.hdf5',\n",
    "                            monitor='val_loss',save_best_only=True,verbose=3)\n",
    "learning_rate1 = ReduceLROnPlateau(monitor= 'val_loss', patience=3, verbose= 3, )\n",
    "\n",
    "callbacks1=[checkpoint1,learning_rate1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "309dc294",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "614/614 [==============================] - ETA: 0s - loss: 0.4528 - accuracy: 0.7826\n",
      "Epoch 1: val_loss improved from inf to 0.44804, saving model to C:\\Users\\vigna\\Desktop\\mini project\\model\\model_vgg19_ensemble.hdf5\n",
      "614/614 [==============================] - 673s 1s/step - loss: 0.4528 - accuracy: 0.7826 - val_loss: 0.4480 - val_accuracy: 0.7981 - lr: 0.0010\n",
      "Epoch 2/20\n",
      "614/614 [==============================] - ETA: 0s - loss: 0.3437 - accuracy: 0.8529\n",
      "Epoch 2: val_loss improved from 0.44804 to 0.44542, saving model to C:\\Users\\vigna\\Desktop\\mini project\\model\\model_vgg19_ensemble.hdf5\n",
      "614/614 [==============================] - 832s 1s/step - loss: 0.3437 - accuracy: 0.8529 - val_loss: 0.4454 - val_accuracy: 0.7740 - lr: 0.0010\n",
      "Epoch 3/20\n",
      "614/614 [==============================] - ETA: 0s - loss: 0.3131 - accuracy: 0.8682\n",
      "Epoch 3: val_loss improved from 0.44542 to 0.42827, saving model to C:\\Users\\vigna\\Desktop\\mini project\\model\\model_vgg19_ensemble.hdf5\n",
      "614/614 [==============================] - 1000s 2s/step - loss: 0.3131 - accuracy: 0.8682 - val_loss: 0.4283 - val_accuracy: 0.7807 - lr: 0.0010\n",
      "Epoch 4/20\n",
      "614/614 [==============================] - ETA: 0s - loss: 0.2986 - accuracy: 0.8741\n",
      "Epoch 4: val_loss improved from 0.42827 to 0.40136, saving model to C:\\Users\\vigna\\Desktop\\mini project\\model\\model_vgg19_ensemble.hdf5\n",
      "614/614 [==============================] - 845s 1s/step - loss: 0.2986 - accuracy: 0.8741 - val_loss: 0.4014 - val_accuracy: 0.8060 - lr: 0.0010\n",
      "Epoch 5/20\n",
      "614/614 [==============================] - ETA: 0s - loss: 0.2984 - accuracy: 0.8731\n",
      "Epoch 5: val_loss improved from 0.40136 to 0.38208, saving model to C:\\Users\\vigna\\Desktop\\mini project\\model\\model_vgg19_ensemble.hdf5\n",
      "614/614 [==============================] - 660s 1s/step - loss: 0.2984 - accuracy: 0.8731 - val_loss: 0.3821 - val_accuracy: 0.8283 - lr: 0.0010\n",
      "Epoch 6/20\n",
      "614/614 [==============================] - ETA: 0s - loss: 0.2853 - accuracy: 0.8792\n",
      "Epoch 6: val_loss did not improve from 0.38208\n",
      "614/614 [==============================] - 653s 1s/step - loss: 0.2853 - accuracy: 0.8792 - val_loss: 0.3902 - val_accuracy: 0.8312 - lr: 0.0010\n",
      "Epoch 7/20\n",
      "614/614 [==============================] - ETA: 0s - loss: 0.2822 - accuracy: 0.8816\n",
      "Epoch 7: val_loss improved from 0.38208 to 0.37448, saving model to C:\\Users\\vigna\\Desktop\\mini project\\model\\model_vgg19_ensemble.hdf5\n",
      "614/614 [==============================] - 662s 1s/step - loss: 0.2822 - accuracy: 0.8816 - val_loss: 0.3745 - val_accuracy: 0.8416 - lr: 0.0010\n",
      "Epoch 8/20\n",
      "614/614 [==============================] - ETA: 0s - loss: 0.2828 - accuracy: 0.8790\n",
      "Epoch 8: val_loss improved from 0.37448 to 0.36463, saving model to C:\\Users\\vigna\\Desktop\\mini project\\model\\model_vgg19_ensemble.hdf5\n",
      "614/614 [==============================] - 606s 987ms/step - loss: 0.2828 - accuracy: 0.8790 - val_loss: 0.3646 - val_accuracy: 0.8449 - lr: 0.0010\n",
      "Epoch 9/20\n",
      "614/614 [==============================] - ETA: 0s - loss: 0.2769 - accuracy: 0.8838\n",
      "Epoch 9: val_loss did not improve from 0.36463\n",
      "614/614 [==============================] - 633s 1s/step - loss: 0.2769 - accuracy: 0.8838 - val_loss: 0.3820 - val_accuracy: 0.8332 - lr: 0.0010\n",
      "Epoch 10/20\n",
      "614/614 [==============================] - ETA: 0s - loss: 0.2756 - accuracy: 0.8810\n",
      "Epoch 10: val_loss did not improve from 0.36463\n",
      "614/614 [==============================] - 642s 1s/step - loss: 0.2756 - accuracy: 0.8810 - val_loss: 0.4266 - val_accuracy: 0.7916 - lr: 0.0010\n",
      "Epoch 11/20\n",
      "614/614 [==============================] - ETA: 0s - loss: 0.2694 - accuracy: 0.8858\n",
      "Epoch 11: val_loss improved from 0.36463 to 0.36382, saving model to C:\\Users\\vigna\\Desktop\\mini project\\model\\model_vgg19_ensemble.hdf5\n",
      "614/614 [==============================] - 619s 1s/step - loss: 0.2694 - accuracy: 0.8858 - val_loss: 0.3638 - val_accuracy: 0.8443 - lr: 0.0010\n",
      "Epoch 12/20\n",
      "614/614 [==============================] - ETA: 0s - loss: 0.2680 - accuracy: 0.8857\n",
      "Epoch 12: val_loss did not improve from 0.36382\n",
      "614/614 [==============================] - 617s 1s/step - loss: 0.2680 - accuracy: 0.8857 - val_loss: 0.3958 - val_accuracy: 0.8358 - lr: 0.0010\n",
      "Epoch 13/20\n",
      "614/614 [==============================] - ETA: 0s - loss: 0.2680 - accuracy: 0.8861\n",
      "Epoch 13: val_loss did not improve from 0.36382\n",
      "614/614 [==============================] - 608s 990ms/step - loss: 0.2680 - accuracy: 0.8861 - val_loss: 0.3913 - val_accuracy: 0.8301 - lr: 0.0010\n",
      "Epoch 14/20\n",
      "614/614 [==============================] - ETA: 0s - loss: 0.2562 - accuracy: 0.8894\n",
      "Epoch 14: val_loss did not improve from 0.36382\n",
      "\n",
      "Epoch 14: ReduceLROnPlateau reducing learning rate to 0.00010000000474974513.\n",
      "614/614 [==============================] - 611s 996ms/step - loss: 0.2562 - accuracy: 0.8894 - val_loss: 0.3837 - val_accuracy: 0.8331 - lr: 0.0010\n",
      "Epoch 15/20\n",
      "614/614 [==============================] - ETA: 0s - loss: 0.2323 - accuracy: 0.9037\n",
      "Epoch 15: val_loss did not improve from 0.36382\n",
      "614/614 [==============================] - 607s 989ms/step - loss: 0.2323 - accuracy: 0.9037 - val_loss: 0.3721 - val_accuracy: 0.8419 - lr: 1.0000e-04\n",
      "Epoch 16/20\n",
      "614/614 [==============================] - ETA: 0s - loss: 0.2252 - accuracy: 0.9085\n",
      "Epoch 16: val_loss did not improve from 0.36382\n",
      "614/614 [==============================] - 607s 989ms/step - loss: 0.2252 - accuracy: 0.9085 - val_loss: 0.3940 - val_accuracy: 0.8260 - lr: 1.0000e-04\n",
      "Epoch 17/20\n",
      "614/614 [==============================] - ETA: 0s - loss: 0.2263 - accuracy: 0.9080\n",
      "Epoch 17: val_loss did not improve from 0.36382\n",
      "\n",
      "Epoch 17: ReduceLROnPlateau reducing learning rate to 1.0000000474974514e-05.\n",
      "614/614 [==============================] - 607s 989ms/step - loss: 0.2263 - accuracy: 0.9080 - val_loss: 0.3887 - val_accuracy: 0.8279 - lr: 1.0000e-04\n",
      "Epoch 18/20\n",
      "614/614 [==============================] - ETA: 0s - loss: 0.2267 - accuracy: 0.9075\n",
      "Epoch 18: val_loss did not improve from 0.36382\n",
      "614/614 [==============================] - 608s 990ms/step - loss: 0.2267 - accuracy: 0.9075 - val_loss: 0.3676 - val_accuracy: 0.8436 - lr: 1.0000e-05\n",
      "Epoch 19/20\n",
      "614/614 [==============================] - ETA: 0s - loss: 0.2267 - accuracy: 0.9080\n",
      "Epoch 19: val_loss did not improve from 0.36382\n",
      "614/614 [==============================] - 606s 987ms/step - loss: 0.2267 - accuracy: 0.9080 - val_loss: 0.3784 - val_accuracy: 0.8357 - lr: 1.0000e-05\n",
      "Epoch 20/20\n",
      "614/614 [==============================] - ETA: 0s - loss: 0.2253 - accuracy: 0.9091\n",
      "Epoch 20: val_loss did not improve from 0.36382\n",
      "\n",
      "Epoch 20: ReduceLROnPlateau reducing learning rate to 1.0000000656873453e-06.\n",
      "614/614 [==============================] - 606s 986ms/step - loss: 0.2253 - accuracy: 0.9091 - val_loss: 0.3663 - val_accuracy: 0.8437 - lr: 1.0000e-05\n"
     ]
    }
   ],
   "source": [
    "model1.compile(optimizer='Adam', loss='categorical_crossentropy',metrics=['accuracy'])\n",
    "history2=model1.fit(train_data,\n",
    "                   validation_data=validation_data,\n",
    "                   callbacks=callbacks1,\n",
    "                    epochs=20)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0de5980",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "4a0b05cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "bmode2 = ResNet50(include_top=False, weights='imagenet', input_tensor=Input(shape=(80,80,3)))\n",
    "hmode2 = bmode2.output\n",
    "hmode2 = Flatten()(hmode2)\n",
    "hmode2 = Dense(64, activation='relu')(hmode2)\n",
    "hmode2 = Dropout(0.5)(hmode2)\n",
    "hmode2 = Dense(2,activation= 'softmax')(hmode2)\n",
    "model3 = Model(inputs=bmode2.input, outputs= hmode2)\n",
    "for layer in bmode2.layers:\n",
    "    layer.trainable = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "5b73d92a",
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint2 = ModelCheckpoint(r'C:\\Users\\vigna\\Desktop\\mini project\\model\\model_ensemble2.hdf5',\n",
    "                            monitor='val_loss',save_best_only=True,verbose=3)\n",
    "\n",
    "learning_rate2 = ReduceLROnPlateau(monitor= 'val_loss', patience=3, verbose= 3 )\n",
    "\n",
    "callbacks2=[checkpoint2,learning_rate2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "4955ac21",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\vigna\\AppData\\Local\\Temp\\ipykernel_26384\\837215024.py:2: UserWarning: `Model.fit_generator` is deprecated and will be removed in a future version. Please use `Model.fit`, which supports generators.\n",
      "  history_=model3.fit_generator(train_data,steps_per_epoch=train_data.samples//batchsize,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "613/613 [==============================] - ETA: 0s - loss: 0.6948 - accuracy: 0.5066\n",
      "Epoch 1: val_loss improved from inf to 0.69303, saving model to C:\\Users\\vigna\\Desktop\\mini project\\model\\model_ensemble2.hdf5\n",
      "613/613 [==============================] - 387s 629ms/step - loss: 0.6948 - accuracy: 0.5066 - val_loss: 0.6930 - val_accuracy: 0.5076 - lr: 0.0010\n",
      "Epoch 2/50\n",
      "613/613 [==============================] - ETA: 0s - loss: 0.6931 - accuracy: 0.5076\n",
      "Epoch 2: val_loss did not improve from 0.69303\n",
      "613/613 [==============================] - 388s 633ms/step - loss: 0.6931 - accuracy: 0.5076 - val_loss: 0.6930 - val_accuracy: 0.5078 - lr: 0.0010\n",
      "Epoch 3/50\n",
      "613/613 [==============================] - ETA: 0s - loss: 0.6931 - accuracy: 0.5077\n",
      "Epoch 3: val_loss did not improve from 0.69303\n",
      "613/613 [==============================] - 401s 654ms/step - loss: 0.6931 - accuracy: 0.5077 - val_loss: 0.6930 - val_accuracy: 0.5076 - lr: 0.0010\n",
      "Epoch 4/50\n",
      "613/613 [==============================] - ETA: 0s - loss: 0.6931 - accuracy: 0.5077\n",
      "Epoch 4: val_loss improved from 0.69303 to 0.69302, saving model to C:\\Users\\vigna\\Desktop\\mini project\\model\\model_ensemble2.hdf5\n",
      "\n",
      "Epoch 4: ReduceLROnPlateau reducing learning rate to 0.00010000000474974513.\n",
      "613/613 [==============================] - 400s 652ms/step - loss: 0.6931 - accuracy: 0.5077 - val_loss: 0.6930 - val_accuracy: 0.5080 - lr: 0.0010\n",
      "Epoch 5/50\n",
      "613/613 [==============================] - ETA: 0s - loss: 0.6930 - accuracy: 0.5077\n",
      "Epoch 5: val_loss did not improve from 0.69302\n",
      "613/613 [==============================] - 400s 652ms/step - loss: 0.6930 - accuracy: 0.5077 - val_loss: 0.6930 - val_accuracy: 0.5078 - lr: 1.0000e-04\n",
      "Epoch 6/50\n",
      "613/613 [==============================] - ETA: 0s - loss: 0.6930 - accuracy: 0.5078\n",
      "Epoch 6: val_loss did not improve from 0.69302\n",
      "613/613 [==============================] - 399s 651ms/step - loss: 0.6930 - accuracy: 0.5078 - val_loss: 0.6930 - val_accuracy: 0.5073 - lr: 1.0000e-04\n",
      "Epoch 7/50\n",
      "613/613 [==============================] - ETA: 0s - loss: 0.6930 - accuracy: 0.5077\n",
      "Epoch 7: val_loss did not improve from 0.69302\n",
      "\n",
      "Epoch 7: ReduceLROnPlateau reducing learning rate to 1.0000000474974514e-05.\n",
      "613/613 [==============================] - 400s 652ms/step - loss: 0.6930 - accuracy: 0.5077 - val_loss: 0.6930 - val_accuracy: 0.5078 - lr: 1.0000e-04\n",
      "Epoch 8/50\n",
      "613/613 [==============================] - ETA: 0s - loss: 0.6930 - accuracy: 0.5078\n",
      "Epoch 8: val_loss did not improve from 0.69302\n",
      "613/613 [==============================] - 400s 653ms/step - loss: 0.6930 - accuracy: 0.5078 - val_loss: 0.6930 - val_accuracy: 0.5075 - lr: 1.0000e-05\n",
      "Epoch 9/50\n",
      "613/613 [==============================] - ETA: 0s - loss: 0.6930 - accuracy: 0.5077\n",
      "Epoch 9: val_loss did not improve from 0.69302\n",
      "613/613 [==============================] - 400s 653ms/step - loss: 0.6930 - accuracy: 0.5077 - val_loss: 0.6930 - val_accuracy: 0.5076 - lr: 1.0000e-05\n",
      "Epoch 10/50\n",
      "613/613 [==============================] - ETA: 0s - loss: 0.6930 - accuracy: 0.5076\n",
      "Epoch 10: val_loss did not improve from 0.69302\n",
      "\n",
      "Epoch 10: ReduceLROnPlateau reducing learning rate to 1.0000000656873453e-06.\n",
      "613/613 [==============================] - 401s 654ms/step - loss: 0.6930 - accuracy: 0.5076 - val_loss: 0.6930 - val_accuracy: 0.5077 - lr: 1.0000e-05\n",
      "Epoch 11/50\n",
      "613/613 [==============================] - ETA: 0s - loss: 0.6930 - accuracy: 0.5075\n",
      "Epoch 11: val_loss did not improve from 0.69302\n",
      "613/613 [==============================] - 401s 653ms/step - loss: 0.6930 - accuracy: 0.5075 - val_loss: 0.6930 - val_accuracy: 0.5078 - lr: 1.0000e-06\n",
      "Epoch 12/50\n",
      "613/613 [==============================] - ETA: 0s - loss: 0.6930 - accuracy: 0.5076\n",
      "Epoch 12: val_loss did not improve from 0.69302\n",
      "613/613 [==============================] - 400s 652ms/step - loss: 0.6930 - accuracy: 0.5076 - val_loss: 0.6930 - val_accuracy: 0.5073 - lr: 1.0000e-06\n",
      "Epoch 13/50\n",
      "613/613 [==============================] - ETA: 0s - loss: 0.6930 - accuracy: 0.5078\n",
      "Epoch 13: val_loss did not improve from 0.69302\n",
      "\n",
      "Epoch 13: ReduceLROnPlateau reducing learning rate to 1.0000001111620805e-07.\n",
      "613/613 [==============================] - 399s 652ms/step - loss: 0.6930 - accuracy: 0.5078 - val_loss: 0.6930 - val_accuracy: 0.5078 - lr: 1.0000e-06\n",
      "Epoch 14/50\n",
      "613/613 [==============================] - ETA: 0s - loss: 0.6930 - accuracy: 0.5077\n",
      "Epoch 14: val_loss did not improve from 0.69302\n",
      "613/613 [==============================] - 400s 652ms/step - loss: 0.6930 - accuracy: 0.5077 - val_loss: 0.6930 - val_accuracy: 0.5074 - lr: 1.0000e-07\n",
      "Epoch 15/50\n",
      "613/613 [==============================] - ETA: 0s - loss: 0.6930 - accuracy: 0.5076\n",
      "Epoch 15: val_loss did not improve from 0.69302\n",
      "613/613 [==============================] - 400s 652ms/step - loss: 0.6930 - accuracy: 0.5076 - val_loss: 0.6930 - val_accuracy: 0.5075 - lr: 1.0000e-07\n",
      "Epoch 16/50\n",
      "613/613 [==============================] - ETA: 0s - loss: 0.6930 - accuracy: 0.5077\n",
      "Epoch 16: val_loss did not improve from 0.69302\n",
      "\n",
      "Epoch 16: ReduceLROnPlateau reducing learning rate to 1.000000082740371e-08.\n",
      "613/613 [==============================] - 400s 652ms/step - loss: 0.6930 - accuracy: 0.5077 - val_loss: 0.6930 - val_accuracy: 0.5076 - lr: 1.0000e-07\n",
      "Epoch 17/50\n",
      "613/613 [==============================] - ETA: 0s - loss: 0.6930 - accuracy: 0.5077\n",
      "Epoch 17: val_loss improved from 0.69302 to 0.69302, saving model to C:\\Users\\vigna\\Desktop\\mini project\\model\\model_ensemble2.hdf5\n",
      "613/613 [==============================] - 400s 652ms/step - loss: 0.6930 - accuracy: 0.5077 - val_loss: 0.6930 - val_accuracy: 0.5080 - lr: 1.0000e-08\n",
      "Epoch 18/50\n",
      "613/613 [==============================] - ETA: 0s - loss: 0.6930 - accuracy: 0.5076\n",
      "Epoch 18: val_loss did not improve from 0.69302\n",
      "613/613 [==============================] - 400s 653ms/step - loss: 0.6930 - accuracy: 0.5076 - val_loss: 0.6930 - val_accuracy: 0.5078 - lr: 1.0000e-08\n",
      "Epoch 19/50\n",
      "613/613 [==============================] - ETA: 0s - loss: 0.6930 - accuracy: 0.5077\n",
      "Epoch 19: val_loss did not improve from 0.69302\n",
      "\n",
      "Epoch 19: ReduceLROnPlateau reducing learning rate to 1.000000082740371e-09.\n",
      "613/613 [==============================] - 379s 618ms/step - loss: 0.6930 - accuracy: 0.5077 - val_loss: 0.6930 - val_accuracy: 0.5074 - lr: 1.0000e-08\n",
      "Epoch 20/50\n",
      "613/613 [==============================] - ETA: 0s - loss: 0.6930 - accuracy: 0.5076\n",
      "Epoch 20: val_loss did not improve from 0.69302\n",
      "613/613 [==============================] - 388s 633ms/step - loss: 0.6930 - accuracy: 0.5076 - val_loss: 0.6930 - val_accuracy: 0.5076 - lr: 1.0000e-09\n",
      "Epoch 21/50\n",
      "613/613 [==============================] - ETA: 0s - loss: 0.6930 - accuracy: 0.5075\n",
      "Epoch 21: val_loss did not improve from 0.69302\n",
      "613/613 [==============================] - 398s 649ms/step - loss: 0.6930 - accuracy: 0.5075 - val_loss: 0.6930 - val_accuracy: 0.5080 - lr: 1.0000e-09\n",
      "Epoch 22/50\n",
      "613/613 [==============================] - ETA: 0s - loss: 0.6930 - accuracy: 0.5077\n",
      "Epoch 22: val_loss did not improve from 0.69302\n",
      "\n",
      "Epoch 22: ReduceLROnPlateau reducing learning rate to 1.000000082740371e-10.\n",
      "613/613 [==============================] - 398s 649ms/step - loss: 0.6930 - accuracy: 0.5077 - val_loss: 0.6930 - val_accuracy: 0.5080 - lr: 1.0000e-09\n",
      "Epoch 23/50\n",
      "613/613 [==============================] - ETA: 0s - loss: 0.6930 - accuracy: 0.5078\n",
      "Epoch 23: val_loss did not improve from 0.69302\n",
      "613/613 [==============================] - 397s 648ms/step - loss: 0.6930 - accuracy: 0.5078 - val_loss: 0.6930 - val_accuracy: 0.5079 - lr: 1.0000e-10\n",
      "Epoch 24/50\n",
      "613/613 [==============================] - ETA: 0s - loss: 0.6930 - accuracy: 0.5077\n",
      "Epoch 24: val_loss did not improve from 0.69302\n",
      "613/613 [==============================] - 397s 648ms/step - loss: 0.6930 - accuracy: 0.5077 - val_loss: 0.6930 - val_accuracy: 0.5076 - lr: 1.0000e-10\n",
      "Epoch 25/50\n",
      "613/613 [==============================] - ETA: 0s - loss: 0.6930 - accuracy: 0.5076\n",
      "Epoch 25: val_loss did not improve from 0.69302\n",
      "\n",
      "Epoch 25: ReduceLROnPlateau reducing learning rate to 1.000000082740371e-11.\n",
      "613/613 [==============================] - 397s 647ms/step - loss: 0.6930 - accuracy: 0.5076 - val_loss: 0.6930 - val_accuracy: 0.5075 - lr: 1.0000e-10\n",
      "Epoch 26/50\n",
      "613/613 [==============================] - ETA: 0s - loss: 0.6930 - accuracy: 0.5077\n",
      "Epoch 26: val_loss did not improve from 0.69302\n",
      "613/613 [==============================] - 394s 642ms/step - loss: 0.6930 - accuracy: 0.5077 - val_loss: 0.6930 - val_accuracy: 0.5080 - lr: 1.0000e-11\n",
      "Epoch 27/50\n",
      "613/613 [==============================] - ETA: 0s - loss: 0.6930 - accuracy: 0.5075\n",
      "Epoch 27: val_loss did not improve from 0.69302\n",
      "613/613 [==============================] - 378s 616ms/step - loss: 0.6930 - accuracy: 0.5075 - val_loss: 0.6930 - val_accuracy: 0.5074 - lr: 1.0000e-11\n",
      "Epoch 28/50\n",
      "613/613 [==============================] - ETA: 0s - loss: 0.6930 - accuracy: 0.5077\n",
      "Epoch 28: val_loss did not improve from 0.69302\n",
      "\n",
      "Epoch 28: ReduceLROnPlateau reducing learning rate to 1.000000082740371e-12.\n",
      "613/613 [==============================] - 391s 638ms/step - loss: 0.6930 - accuracy: 0.5077 - val_loss: 0.6930 - val_accuracy: 0.5073 - lr: 1.0000e-11\n",
      "Epoch 29/50\n",
      "613/613 [==============================] - ETA: 0s - loss: 0.6930 - accuracy: 0.5076\n",
      "Epoch 29: val_loss did not improve from 0.69302\n",
      "613/613 [==============================] - 387s 631ms/step - loss: 0.6930 - accuracy: 0.5076 - val_loss: 0.6930 - val_accuracy: 0.5073 - lr: 1.0000e-12\n",
      "Epoch 30/50\n",
      "613/613 [==============================] - ETA: 0s - loss: 0.6930 - accuracy: 0.5076\n",
      "Epoch 30: val_loss did not improve from 0.69302\n",
      "613/613 [==============================] - 391s 638ms/step - loss: 0.6930 - accuracy: 0.5076 - val_loss: 0.6930 - val_accuracy: 0.5078 - lr: 1.0000e-12\n",
      "Epoch 31/50\n",
      "613/613 [==============================] - ETA: 0s - loss: 0.6930 - accuracy: 0.5077\n",
      "Epoch 31: val_loss did not improve from 0.69302\n",
      "\n",
      "Epoch 31: ReduceLROnPlateau reducing learning rate to 1.0000001044244145e-13.\n",
      "613/613 [==============================] - 388s 633ms/step - loss: 0.6930 - accuracy: 0.5077 - val_loss: 0.6930 - val_accuracy: 0.5079 - lr: 1.0000e-12\n",
      "Epoch 32/50\n",
      "613/613 [==============================] - ETA: 0s - loss: 0.6930 - accuracy: 0.5076\n",
      "Epoch 32: val_loss did not improve from 0.69302\n",
      "613/613 [==============================] - 378s 616ms/step - loss: 0.6930 - accuracy: 0.5076 - val_loss: 0.6930 - val_accuracy: 0.5076 - lr: 1.0000e-13\n",
      "Epoch 33/50\n",
      "613/613 [==============================] - ETA: 0s - loss: 0.6930 - accuracy: 0.5076\n",
      "Epoch 33: val_loss did not improve from 0.69302\n",
      "613/613 [==============================] - 398s 649ms/step - loss: 0.6930 - accuracy: 0.5076 - val_loss: 0.6930 - val_accuracy: 0.5079 - lr: 1.0000e-13\n",
      "Epoch 34/50\n",
      "613/613 [==============================] - ETA: 0s - loss: 0.6930 - accuracy: 0.5076\n",
      "Epoch 34: val_loss did not improve from 0.69302\n",
      "\n",
      "Epoch 34: ReduceLROnPlateau reducing learning rate to 1.0000001179769417e-14.\n",
      "613/613 [==============================] - 397s 648ms/step - loss: 0.6930 - accuracy: 0.5076 - val_loss: 0.6930 - val_accuracy: 0.5076 - lr: 1.0000e-13\n",
      "Epoch 35/50\n",
      "613/613 [==============================] - ETA: 0s - loss: 0.6930 - accuracy: 0.5077\n",
      "Epoch 35: val_loss did not improve from 0.69302\n",
      "613/613 [==============================] - 399s 651ms/step - loss: 0.6930 - accuracy: 0.5077 - val_loss: 0.6930 - val_accuracy: 0.5071 - lr: 1.0000e-14\n",
      "Epoch 36/50\n",
      "613/613 [==============================] - ETA: 0s - loss: 0.6930 - accuracy: 0.5077\n",
      "Epoch 36: val_loss did not improve from 0.69302\n",
      "613/613 [==============================] - 399s 651ms/step - loss: 0.6930 - accuracy: 0.5077 - val_loss: 0.6930 - val_accuracy: 0.5078 - lr: 1.0000e-14\n",
      "Epoch 37/50\n",
      "613/613 [==============================] - ETA: 0s - loss: 0.6930 - accuracy: 0.5077\n",
      "Epoch 37: val_loss did not improve from 0.69302\n",
      "\n",
      "Epoch 37: ReduceLROnPlateau reducing learning rate to 1.0000001518582595e-15.\n",
      "613/613 [==============================] - 394s 643ms/step - loss: 0.6930 - accuracy: 0.5077 - val_loss: 0.6930 - val_accuracy: 0.5073 - lr: 1.0000e-14\n",
      "Epoch 38/50\n",
      "613/613 [==============================] - ETA: 0s - loss: 0.6930 - accuracy: 0.5076\n",
      "Epoch 38: val_loss did not improve from 0.69302\n",
      "613/613 [==============================] - 378s 617ms/step - loss: 0.6930 - accuracy: 0.5076 - val_loss: 0.6930 - val_accuracy: 0.5078 - lr: 1.0000e-15\n",
      "Epoch 39/50\n",
      "613/613 [==============================] - ETA: 0s - loss: 0.6930 - accuracy: 0.5077\n",
      "Epoch 39: val_loss improved from 0.69302 to 0.69302, saving model to C:\\Users\\vigna\\Desktop\\mini project\\model\\model_ensemble2.hdf5\n",
      "613/613 [==============================] - 375s 611ms/step - loss: 0.6930 - accuracy: 0.5077 - val_loss: 0.6930 - val_accuracy: 0.5081 - lr: 1.0000e-15\n",
      "Epoch 40/50\n",
      "613/613 [==============================] - ETA: 0s - loss: 0.6930 - accuracy: 0.5077\n",
      "Epoch 40: val_loss did not improve from 0.69302\n",
      "\n",
      "Epoch 40: ReduceLROnPlateau reducing learning rate to 1.0000001095066122e-16.\n",
      "613/613 [==============================] - 377s 615ms/step - loss: 0.6930 - accuracy: 0.5077 - val_loss: 0.6930 - val_accuracy: 0.5076 - lr: 1.0000e-15\n",
      "Epoch 41/50\n",
      "613/613 [==============================] - ETA: 0s - loss: 0.6930 - accuracy: 0.5076\n",
      "Epoch 41: val_loss did not improve from 0.69302\n",
      "613/613 [==============================] - 375s 612ms/step - loss: 0.6930 - accuracy: 0.5076 - val_loss: 0.6930 - val_accuracy: 0.5077 - lr: 1.0000e-16\n",
      "Epoch 42/50\n",
      "613/613 [==============================] - ETA: 0s - loss: 0.6930 - accuracy: 0.5077\n",
      "Epoch 42: val_loss did not improve from 0.69302\n",
      "613/613 [==============================] - 373s 608ms/step - loss: 0.6930 - accuracy: 0.5077 - val_loss: 0.6930 - val_accuracy: 0.5077 - lr: 1.0000e-16\n",
      "Epoch 43/50\n",
      "613/613 [==============================] - ETA: 0s - loss: 0.6930 - accuracy: 0.5076\n",
      "Epoch 43: val_loss did not improve from 0.69302\n",
      "\n",
      "Epoch 43: ReduceLROnPlateau reducing learning rate to 1.0000000830368326e-17.\n",
      "613/613 [==============================] - 372s 607ms/step - loss: 0.6930 - accuracy: 0.5076 - val_loss: 0.6930 - val_accuracy: 0.5075 - lr: 1.0000e-16\n",
      "Epoch 44/50\n",
      "613/613 [==============================] - ETA: 0s - loss: 0.6930 - accuracy: 0.5077\n",
      "Epoch 44: val_loss did not improve from 0.69302\n",
      "613/613 [==============================] - 373s 608ms/step - loss: 0.6930 - accuracy: 0.5077 - val_loss: 0.6930 - val_accuracy: 0.5077 - lr: 1.0000e-17\n",
      "Epoch 45/50\n",
      "613/613 [==============================] - ETA: 0s - loss: 0.6930 - accuracy: 0.5077\n",
      "Epoch 45: val_loss did not improve from 0.69302\n",
      "613/613 [==============================] - 384s 627ms/step - loss: 0.6930 - accuracy: 0.5077 - val_loss: 0.6930 - val_accuracy: 0.5078 - lr: 1.0000e-17\n",
      "Epoch 46/50\n",
      "613/613 [==============================] - ETA: 0s - loss: 0.6930 - accuracy: 0.5077\n",
      "Epoch 46: val_loss did not improve from 0.69302\n",
      "\n",
      "Epoch 46: ReduceLROnPlateau reducing learning rate to 1.0000000664932204e-18.\n",
      "613/613 [==============================] - 384s 626ms/step - loss: 0.6930 - accuracy: 0.5077 - val_loss: 0.6930 - val_accuracy: 0.5079 - lr: 1.0000e-17\n",
      "Epoch 47/50\n",
      "613/613 [==============================] - ETA: 0s - loss: 0.6930 - accuracy: 0.5076\n",
      "Epoch 47: val_loss did not improve from 0.69302\n",
      "613/613 [==============================] - 381s 622ms/step - loss: 0.6930 - accuracy: 0.5076 - val_loss: 0.6930 - val_accuracy: 0.5077 - lr: 1.0000e-18\n",
      "Epoch 48/50\n",
      "613/613 [==============================] - ETA: 0s - loss: 0.6930 - accuracy: 0.5076\n",
      "Epoch 48: val_loss did not improve from 0.69302\n",
      "613/613 [==============================] - 381s 621ms/step - loss: 0.6930 - accuracy: 0.5076 - val_loss: 0.6930 - val_accuracy: 0.5074 - lr: 1.0000e-18\n",
      "Epoch 49/50\n",
      "613/613 [==============================] - ETA: 0s - loss: 0.6930 - accuracy: 0.5076\n",
      "Epoch 49: val_loss did not improve from 0.69302\n",
      "\n",
      "Epoch 49: ReduceLROnPlateau reducing learning rate to 1.000000045813705e-19.\n",
      "613/613 [==============================] - 381s 622ms/step - loss: 0.6930 - accuracy: 0.5076 - val_loss: 0.6930 - val_accuracy: 0.5075 - lr: 1.0000e-18\n",
      "Epoch 50/50\n",
      "613/613 [==============================] - ETA: 0s - loss: 0.6930 - accuracy: 0.5078\n",
      "Epoch 50: val_loss did not improve from 0.69302\n",
      "613/613 [==============================] - 382s 623ms/step - loss: 0.6930 - accuracy: 0.5078 - val_loss: 0.6930 - val_accuracy: 0.5075 - lr: 1.0000e-19\n"
     ]
    }
   ],
   "source": [
    "model3.compile(optimizer='Adam', loss='categorical_crossentropy',metrics=['accuracy'])\n",
    "history_=model3.fit_generator(train_data,steps_per_epoch=train_data.samples//batchsize,\n",
    "                   validation_data=validation_data,\n",
    "                   validation_steps=validation_data.samples//batchsize,\n",
    "                   callbacks=callbacks2,\n",
    "                    epochs=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "ef3ca7dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Model, load_model\n",
    "from tensorflow.keras.layers import Input, Average\n",
    "\n",
    "mode = load_model(r'C:\\Users\\vigna\\Desktop\\mini project\\model\\model_ensemble.hdf5')\n",
    "model_1 = Model(inputs=model.inputs,\n",
    "                outputs=model.outputs,\n",
    "                name='name_of_model_1') \n",
    "model_2 = load_model(r'C:\\Users\\vigna\\Desktop\\mini project\\model\\model_vgg19_ensemble.hdf5')\n",
    "model_2 = Model(inputs=model1.inputs,\n",
    "                outputs=model1.outputs,\n",
    "                name='name_of_model_2')\n",
    "model_3 = load_model(r'C:\\Users\\vigna\\Desktop\\mini project\\model\\model_ensemble2.hdf5')\n",
    "model_3 = Model(inputs=model3.inputs,\n",
    "                outputs=model3.outputs,\n",
    "                name='name_of_model_3')\n",
    "models = [model_1, model_2,model_3]\n",
    "model_input = Input(shape=(80, 80, 3))\n",
    "model_outputs = [model(model_input) for model in models]\n",
    "ensemble_output = Average()(model_outputs)\n",
    "ensemble_model = Model(inputs=model_input, outputs=ensemble_output, name='ensemble')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "d42898d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpointe = ModelCheckpoint(r'C:\\Users\\vigna\\Desktop\\mini project\\model\\model_ensemblefin.hdf5',\n",
    "                            monitor='val_loss',save_best_only=True,verbose=3)\n",
    "\n",
    "learning_ratee = ReduceLROnPlateau(monitor= 'val_loss', patience=3, verbose= 3 )\n",
    "\n",
    "callbackse=[checkpointe,learning_ratee]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be70286c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "f4645a74",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "614/614 [==============================] - ETA: 0s - loss: 0.2850 - accuracy: 0.9449\n",
      "Epoch 1: val_loss improved from inf to 0.31247, saving model to C:\\Users\\vigna\\Desktop\\mini project\\model\\model_ensemblefin.hdf5\n",
      "614/614 [==============================] - 1066s 2s/step - loss: 0.2850 - accuracy: 0.9449 - val_loss: 0.3125 - val_accuracy: 0.9317 - lr: 0.0010\n",
      "Epoch 2/50\n",
      "614/614 [==============================] - ETA: 0s - loss: 0.2815 - accuracy: 0.9451\n",
      "Epoch 2: val_loss did not improve from 0.31247\n",
      "614/614 [==============================] - 1057s 2s/step - loss: 0.2815 - accuracy: 0.9451 - val_loss: 0.3245 - val_accuracy: 0.9181 - lr: 0.0010\n",
      "Epoch 3/50\n",
      "614/614 [==============================] - ETA: 0s - loss: 0.2814 - accuracy: 0.9425\n",
      "Epoch 3: val_loss did not improve from 0.31247\n",
      "614/614 [==============================] - 1112s 2s/step - loss: 0.2814 - accuracy: 0.9425 - val_loss: 0.3190 - val_accuracy: 0.9161 - lr: 0.0010\n",
      "Epoch 4/50\n",
      "614/614 [==============================] - ETA: 0s - loss: 0.2798 - accuracy: 0.9429\n",
      "Epoch 4: val_loss did not improve from 0.31247\n",
      "\n",
      "Epoch 4: ReduceLROnPlateau reducing learning rate to 0.00010000000474974513.\n",
      "614/614 [==============================] - 1054s 2s/step - loss: 0.2798 - accuracy: 0.9429 - val_loss: 0.3193 - val_accuracy: 0.9102 - lr: 0.0010\n",
      "Epoch 5/50\n",
      "614/614 [==============================] - ETA: 0s - loss: 0.2695 - accuracy: 0.9470\n",
      "Epoch 5: val_loss did not improve from 0.31247\n",
      "614/614 [==============================] - 1060s 2s/step - loss: 0.2695 - accuracy: 0.9470 - val_loss: 0.3314 - val_accuracy: 0.8970 - lr: 1.0000e-04\n",
      "Epoch 6/50\n",
      "614/614 [==============================] - ETA: 0s - loss: 0.2668 - accuracy: 0.9487\n",
      "Epoch 6: val_loss did not improve from 0.31247\n",
      "614/614 [==============================] - 1046s 2s/step - loss: 0.2668 - accuracy: 0.9487 - val_loss: 0.3193 - val_accuracy: 0.9117 - lr: 1.0000e-04\n",
      "Epoch 7/50\n",
      "614/614 [==============================] - ETA: 0s - loss: 0.2669 - accuracy: 0.9489\n",
      "Epoch 7: val_loss did not improve from 0.31247\n",
      "\n",
      "Epoch 7: ReduceLROnPlateau reducing learning rate to 1.0000000474974514e-05.\n",
      "614/614 [==============================] - 1046s 2s/step - loss: 0.2669 - accuracy: 0.9489 - val_loss: 0.3228 - val_accuracy: 0.9079 - lr: 1.0000e-04\n",
      "Epoch 8/50\n",
      "614/614 [==============================] - ETA: 0s - loss: 0.2664 - accuracy: 0.9479\n",
      "Epoch 8: val_loss did not improve from 0.31247\n",
      "614/614 [==============================] - 1046s 2s/step - loss: 0.2664 - accuracy: 0.9479 - val_loss: 0.3221 - val_accuracy: 0.9068 - lr: 1.0000e-05\n",
      "Epoch 9/50\n",
      "614/614 [==============================] - ETA: 0s - loss: 0.2668 - accuracy: 0.9484\n",
      "Epoch 9: val_loss did not improve from 0.31247\n",
      "614/614 [==============================] - 1045s 2s/step - loss: 0.2668 - accuracy: 0.9484 - val_loss: 0.3191 - val_accuracy: 0.9108 - lr: 1.0000e-05\n",
      "Epoch 10/50\n",
      "614/614 [==============================] - ETA: 0s - loss: 0.2650 - accuracy: 0.9498\n",
      "Epoch 10: val_loss did not improve from 0.31247\n",
      "\n",
      "Epoch 10: ReduceLROnPlateau reducing learning rate to 1.0000000656873453e-06.\n",
      "614/614 [==============================] - 1044s 2s/step - loss: 0.2650 - accuracy: 0.9498 - val_loss: 0.3215 - val_accuracy: 0.9094 - lr: 1.0000e-05\n",
      "Epoch 11/50\n",
      "614/614 [==============================] - ETA: 0s - loss: 0.2655 - accuracy: 0.9492\n",
      "Epoch 11: val_loss did not improve from 0.31247\n",
      "614/614 [==============================] - 1052s 2s/step - loss: 0.2655 - accuracy: 0.9492 - val_loss: 0.3207 - val_accuracy: 0.9128 - lr: 1.0000e-06\n",
      "Epoch 12/50\n",
      "614/614 [==============================] - ETA: 0s - loss: 0.2657 - accuracy: 0.9503\n",
      "Epoch 12: val_loss did not improve from 0.31247\n",
      "614/614 [==============================] - 1055s 2s/step - loss: 0.2657 - accuracy: 0.9503 - val_loss: 0.3229 - val_accuracy: 0.9061 - lr: 1.0000e-06\n",
      "Epoch 13/50\n",
      "614/614 [==============================] - ETA: 0s - loss: 0.2655 - accuracy: 0.9494\n",
      "Epoch 13: val_loss did not improve from 0.31247\n",
      "\n",
      "Epoch 13: ReduceLROnPlateau reducing learning rate to 1.0000001111620805e-07.\n",
      "614/614 [==============================] - 1070s 2s/step - loss: 0.2655 - accuracy: 0.9494 - val_loss: 0.3215 - val_accuracy: 0.9079 - lr: 1.0000e-06\n",
      "Epoch 14/50\n",
      "614/614 [==============================] - ETA: 0s - loss: 0.2652 - accuracy: 0.9502\n",
      "Epoch 14: val_loss did not improve from 0.31247\n",
      "614/614 [==============================] - 1059s 2s/step - loss: 0.2652 - accuracy: 0.9502 - val_loss: 0.3239 - val_accuracy: 0.9052 - lr: 1.0000e-07\n",
      "Epoch 15/50\n",
      "614/614 [==============================] - ETA: 0s - loss: 0.2654 - accuracy: 0.9502\n",
      "Epoch 15: val_loss did not improve from 0.31247\n",
      "614/614 [==============================] - 1062s 2s/step - loss: 0.2654 - accuracy: 0.9502 - val_loss: 0.3238 - val_accuracy: 0.9056 - lr: 1.0000e-07\n",
      "Epoch 16/50\n",
      "614/614 [==============================] - ETA: 0s - loss: 0.2642 - accuracy: 0.9506\n",
      "Epoch 16: val_loss did not improve from 0.31247\n",
      "\n",
      "Epoch 16: ReduceLROnPlateau reducing learning rate to 1.000000082740371e-08.\n",
      "614/614 [==============================] - 1059s 2s/step - loss: 0.2642 - accuracy: 0.9506 - val_loss: 0.3204 - val_accuracy: 0.9101 - lr: 1.0000e-07\n",
      "Epoch 17/50\n",
      "614/614 [==============================] - ETA: 0s - loss: 0.2664 - accuracy: 0.9491\n",
      "Epoch 17: val_loss did not improve from 0.31247\n",
      "614/614 [==============================] - 1080s 2s/step - loss: 0.2664 - accuracy: 0.9491 - val_loss: 0.3224 - val_accuracy: 0.9082 - lr: 1.0000e-08\n",
      "Epoch 18/50\n",
      "614/614 [==============================] - ETA: 0s - loss: 0.2661 - accuracy: 0.9481\n",
      "Epoch 18: val_loss did not improve from 0.31247\n",
      "614/614 [==============================] - 1147s 2s/step - loss: 0.2661 - accuracy: 0.9481 - val_loss: 0.3218 - val_accuracy: 0.9095 - lr: 1.0000e-08\n",
      "Epoch 19/50\n",
      "614/614 [==============================] - ETA: 0s - loss: 0.2657 - accuracy: 0.9495\n",
      "Epoch 19: val_loss did not improve from 0.31247\n",
      "\n",
      "Epoch 19: ReduceLROnPlateau reducing learning rate to 1.000000082740371e-09.\n",
      "614/614 [==============================] - 1155s 2s/step - loss: 0.2657 - accuracy: 0.9495 - val_loss: 0.3227 - val_accuracy: 0.9082 - lr: 1.0000e-08\n",
      "Epoch 20/50\n",
      "614/614 [==============================] - ETA: 0s - loss: 0.2647 - accuracy: 0.9500\n",
      "Epoch 20: val_loss did not improve from 0.31247\n",
      "614/614 [==============================] - 1147s 2s/step - loss: 0.2647 - accuracy: 0.9500 - val_loss: 0.3210 - val_accuracy: 0.9083 - lr: 1.0000e-09\n",
      "Epoch 21/50\n",
      "614/614 [==============================] - ETA: 0s - loss: 0.2652 - accuracy: 0.9506\n",
      "Epoch 21: val_loss did not improve from 0.31247\n",
      "614/614 [==============================] - 1110s 2s/step - loss: 0.2652 - accuracy: 0.9506 - val_loss: 0.3218 - val_accuracy: 0.9089 - lr: 1.0000e-09\n",
      "Epoch 22/50\n",
      "614/614 [==============================] - ETA: 0s - loss: 0.2649 - accuracy: 0.9506\n",
      "Epoch 22: val_loss did not improve from 0.31247\n",
      "\n",
      "Epoch 22: ReduceLROnPlateau reducing learning rate to 1.000000082740371e-10.\n",
      "614/614 [==============================] - 1071s 2s/step - loss: 0.2649 - accuracy: 0.9506 - val_loss: 0.3220 - val_accuracy: 0.9086 - lr: 1.0000e-09\n",
      "Epoch 23/50\n",
      "614/614 [==============================] - ETA: 0s - loss: 0.2641 - accuracy: 0.9506\n",
      "Epoch 23: val_loss did not improve from 0.31247\n",
      "614/614 [==============================] - 1068s 2s/step - loss: 0.2641 - accuracy: 0.9506 - val_loss: 0.3218 - val_accuracy: 0.9074 - lr: 1.0000e-10\n",
      "Epoch 24/50\n",
      "614/614 [==============================] - ETA: 0s - loss: 0.2661 - accuracy: 0.9490\n",
      "Epoch 24: val_loss did not improve from 0.31247\n",
      "614/614 [==============================] - 2072s 3s/step - loss: 0.2661 - accuracy: 0.9490 - val_loss: 0.3221 - val_accuracy: 0.9078 - lr: 1.0000e-10\n",
      "Epoch 25/50\n",
      "614/614 [==============================] - ETA: 0s - loss: 0.2654 - accuracy: 0.9499\n",
      "Epoch 25: val_loss did not improve from 0.31247\n",
      "\n",
      "Epoch 25: ReduceLROnPlateau reducing learning rate to 1.000000082740371e-11.\n",
      "614/614 [==============================] - 1072s 2s/step - loss: 0.2654 - accuracy: 0.9499 - val_loss: 0.3248 - val_accuracy: 0.9065 - lr: 1.0000e-10\n",
      "Epoch 26/50\n",
      "614/614 [==============================] - ETA: 0s - loss: 0.2660 - accuracy: 0.9489\n",
      "Epoch 26: val_loss did not improve from 0.31247\n",
      "614/614 [==============================] - 1069s 2s/step - loss: 0.2660 - accuracy: 0.9489 - val_loss: 0.3205 - val_accuracy: 0.9101 - lr: 1.0000e-11\n",
      "Epoch 27/50\n",
      "614/614 [==============================] - ETA: 0s - loss: 0.2651 - accuracy: 0.9505\n",
      "Epoch 27: val_loss did not improve from 0.31247\n",
      "614/614 [==============================] - 1122s 2s/step - loss: 0.2651 - accuracy: 0.9505 - val_loss: 0.3218 - val_accuracy: 0.9067 - lr: 1.0000e-11\n",
      "Epoch 28/50\n",
      "614/614 [==============================] - ETA: 0s - loss: 0.2643 - accuracy: 0.9508\n",
      "Epoch 28: val_loss did not improve from 0.31247\n",
      "\n",
      "Epoch 28: ReduceLROnPlateau reducing learning rate to 1.000000082740371e-12.\n",
      "614/614 [==============================] - 1122s 2s/step - loss: 0.2643 - accuracy: 0.9508 - val_loss: 0.3226 - val_accuracy: 0.9084 - lr: 1.0000e-11\n",
      "Epoch 29/50\n",
      "614/614 [==============================] - ETA: 0s - loss: 0.2654 - accuracy: 0.9495\n",
      "Epoch 29: val_loss did not improve from 0.31247\n",
      "614/614 [==============================] - 1158s 2s/step - loss: 0.2654 - accuracy: 0.9495 - val_loss: 0.3197 - val_accuracy: 0.9110 - lr: 1.0000e-12\n",
      "Epoch 30/50\n",
      "614/614 [==============================] - ETA: 0s - loss: 0.2656 - accuracy: 0.9490\n",
      "Epoch 30: val_loss did not improve from 0.31247\n",
      "614/614 [==============================] - 1142s 2s/step - loss: 0.2656 - accuracy: 0.9490 - val_loss: 0.3174 - val_accuracy: 0.9119 - lr: 1.0000e-12\n",
      "Epoch 31/50\n",
      "614/614 [==============================] - ETA: 0s - loss: 0.2654 - accuracy: 0.9504\n",
      "Epoch 31: val_loss did not improve from 0.31247\n",
      "\n",
      "Epoch 31: ReduceLROnPlateau reducing learning rate to 1.0000001044244145e-13.\n",
      "614/614 [==============================] - 1087s 2s/step - loss: 0.2654 - accuracy: 0.9504 - val_loss: 0.3224 - val_accuracy: 0.9081 - lr: 1.0000e-12\n",
      "Epoch 32/50\n",
      "614/614 [==============================] - ETA: 0s - loss: 0.2654 - accuracy: 0.9495\n",
      "Epoch 32: val_loss did not improve from 0.31247\n",
      "614/614 [==============================] - 1128s 2s/step - loss: 0.2654 - accuracy: 0.9495 - val_loss: 0.3240 - val_accuracy: 0.9062 - lr: 1.0000e-13\n",
      "Epoch 33/50\n",
      "614/614 [==============================] - ETA: 0s - loss: 0.2660 - accuracy: 0.9490\n",
      "Epoch 33: val_loss did not improve from 0.31247\n",
      "614/614 [==============================] - 1227s 2s/step - loss: 0.2660 - accuracy: 0.9490 - val_loss: 0.3229 - val_accuracy: 0.9082 - lr: 1.0000e-13\n",
      "Epoch 34/50\n",
      "614/614 [==============================] - ETA: 0s - loss: 0.2657 - accuracy: 0.9487\n",
      "Epoch 34: val_loss did not improve from 0.31247\n",
      "\n",
      "Epoch 34: ReduceLROnPlateau reducing learning rate to 1.0000001179769417e-14.\n",
      "614/614 [==============================] - 1127s 2s/step - loss: 0.2657 - accuracy: 0.9487 - val_loss: 0.3254 - val_accuracy: 0.9061 - lr: 1.0000e-13\n",
      "Epoch 35/50\n",
      "614/614 [==============================] - ETA: 0s - loss: 0.2646 - accuracy: 0.9501\n",
      "Epoch 35: val_loss did not improve from 0.31247\n",
      "614/614 [==============================] - 1214s 2s/step - loss: 0.2646 - accuracy: 0.9501 - val_loss: 0.3236 - val_accuracy: 0.9077 - lr: 1.0000e-14\n",
      "Epoch 36/50\n",
      "614/614 [==============================] - ETA: 0s - loss: 0.2652 - accuracy: 0.9492\n",
      "Epoch 36: val_loss did not improve from 0.31247\n",
      "614/614 [==============================] - 1248s 2s/step - loss: 0.2652 - accuracy: 0.9492 - val_loss: 0.3203 - val_accuracy: 0.9097 - lr: 1.0000e-14\n",
      "Epoch 37/50\n",
      "614/614 [==============================] - ETA: 0s - loss: 0.2658 - accuracy: 0.9491\n",
      "Epoch 37: val_loss did not improve from 0.31247\n",
      "\n",
      "Epoch 37: ReduceLROnPlateau reducing learning rate to 1.0000001518582595e-15.\n",
      "614/614 [==============================] - 1151s 2s/step - loss: 0.2658 - accuracy: 0.9491 - val_loss: 0.3206 - val_accuracy: 0.9099 - lr: 1.0000e-14\n",
      "Epoch 38/50\n",
      "614/614 [==============================] - ETA: 0s - loss: 0.2647 - accuracy: 0.9505\n",
      "Epoch 38: val_loss did not improve from 0.31247\n",
      "614/614 [==============================] - 1253s 2s/step - loss: 0.2647 - accuracy: 0.9505 - val_loss: 0.3235 - val_accuracy: 0.9041 - lr: 1.0000e-15\n",
      "Epoch 39/50\n",
      "614/614 [==============================] - ETA: 0s - loss: 0.2659 - accuracy: 0.9491\n",
      "Epoch 39: val_loss did not improve from 0.31247\n",
      "614/614 [==============================] - 1123s 2s/step - loss: 0.2659 - accuracy: 0.9491 - val_loss: 0.3218 - val_accuracy: 0.9045 - lr: 1.0000e-15\n",
      "Epoch 40/50\n",
      "614/614 [==============================] - ETA: 0s - loss: 0.2640 - accuracy: 0.9507\n",
      "Epoch 40: val_loss did not improve from 0.31247\n",
      "\n",
      "Epoch 40: ReduceLROnPlateau reducing learning rate to 1.0000001095066122e-16.\n",
      "614/614 [==============================] - 1105s 2s/step - loss: 0.2640 - accuracy: 0.9507 - val_loss: 0.3216 - val_accuracy: 0.9060 - lr: 1.0000e-15\n",
      "Epoch 41/50\n",
      "614/614 [==============================] - ETA: 0s - loss: 0.2653 - accuracy: 0.9503\n",
      "Epoch 41: val_loss did not improve from 0.31247\n",
      "614/614 [==============================] - 1089s 2s/step - loss: 0.2653 - accuracy: 0.9503 - val_loss: 0.3259 - val_accuracy: 0.9052 - lr: 1.0000e-16\n",
      "Epoch 42/50\n",
      "614/614 [==============================] - ETA: 0s - loss: 0.2646 - accuracy: 0.9502\n",
      "Epoch 42: val_loss did not improve from 0.31247\n",
      "614/614 [==============================] - 1094s 2s/step - loss: 0.2646 - accuracy: 0.9502 - val_loss: 0.3211 - val_accuracy: 0.9108 - lr: 1.0000e-16\n",
      "Epoch 43/50\n",
      "614/614 [==============================] - ETA: 0s - loss: 0.2648 - accuracy: 0.9495\n",
      "Epoch 43: val_loss did not improve from 0.31247\n",
      "\n",
      "Epoch 43: ReduceLROnPlateau reducing learning rate to 1.0000000830368326e-17.\n",
      "614/614 [==============================] - 1083s 2s/step - loss: 0.2648 - accuracy: 0.9495 - val_loss: 0.3232 - val_accuracy: 0.9073 - lr: 1.0000e-16\n",
      "Epoch 44/50\n",
      "614/614 [==============================] - ETA: 0s - loss: 0.2663 - accuracy: 0.9486\n",
      "Epoch 44: val_loss did not improve from 0.31247\n",
      "614/614 [==============================] - 1125s 2s/step - loss: 0.2663 - accuracy: 0.9486 - val_loss: 0.3212 - val_accuracy: 0.9074 - lr: 1.0000e-17\n",
      "Epoch 45/50\n",
      "614/614 [==============================] - ETA: 0s - loss: 0.2648 - accuracy: 0.9499\n",
      "Epoch 45: val_loss did not improve from 0.31247\n",
      "614/614 [==============================] - 1264s 2s/step - loss: 0.2648 - accuracy: 0.9499 - val_loss: 0.3240 - val_accuracy: 0.9062 - lr: 1.0000e-17\n",
      "Epoch 46/50\n",
      "614/614 [==============================] - ETA: 0s - loss: 0.2651 - accuracy: 0.9502\n",
      "Epoch 46: val_loss did not improve from 0.31247\n",
      "\n",
      "Epoch 46: ReduceLROnPlateau reducing learning rate to 1.0000000664932204e-18.\n",
      "614/614 [==============================] - 1241s 2s/step - loss: 0.2651 - accuracy: 0.9502 - val_loss: 0.3219 - val_accuracy: 0.9078 - lr: 1.0000e-17\n",
      "Epoch 47/50\n",
      "614/614 [==============================] - ETA: 0s - loss: 0.2640 - accuracy: 0.9510\n",
      "Epoch 47: val_loss did not improve from 0.31247\n",
      "614/614 [==============================] - 1188s 2s/step - loss: 0.2640 - accuracy: 0.9510 - val_loss: 0.3237 - val_accuracy: 0.9060 - lr: 1.0000e-18\n",
      "Epoch 48/50\n",
      "614/614 [==============================] - ETA: 0s - loss: 0.2669 - accuracy: 0.9482\n",
      "Epoch 48: val_loss did not improve from 0.31247\n",
      "614/614 [==============================] - 1111s 2s/step - loss: 0.2669 - accuracy: 0.9482 - val_loss: 0.3218 - val_accuracy: 0.9091 - lr: 1.0000e-18\n",
      "Epoch 49/50\n",
      "614/614 [==============================] - ETA: 0s - loss: 0.2653 - accuracy: 0.9498\n",
      "Epoch 49: val_loss did not improve from 0.31247\n",
      "\n",
      "Epoch 49: ReduceLROnPlateau reducing learning rate to 1.000000045813705e-19.\n",
      "614/614 [==============================] - 1071s 2s/step - loss: 0.2653 - accuracy: 0.9498 - val_loss: 0.3233 - val_accuracy: 0.9033 - lr: 1.0000e-18\n",
      "Epoch 50/50\n",
      "614/614 [==============================] - ETA: 0s - loss: 0.2649 - accuracy: 0.9499\n",
      "Epoch 50: val_loss did not improve from 0.31247\n",
      "614/614 [==============================] - 1069s 2s/step - loss: 0.2649 - accuracy: 0.9499 - val_loss: 0.3224 - val_accuracy: 0.9107 - lr: 1.0000e-19\n"
     ]
    }
   ],
   "source": [
    "ensemble_model.compile(optimizer='Adam', loss='categorical_crossentropy',metrics=['accuracy'])\n",
    "history3=ensemble_model.fit(train_data,\n",
    "                   validation_data=validation_data,\n",
    "                   callbacks=callbackse,\n",
    "                    epochs=50)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "066dffe1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\vigna\\AppData\\Local\\Temp\\ipykernel_26384\\1716953310.py:1: UserWarning: `Model.evaluate_generator` is deprecated and will be removed in a future version. Please use `Model.evaluate`, which supports generators.\n",
      "  acc_tr, loss_tr = ensemble_model.evaluate_generator(train_data)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.2581940293312073\n",
      "0.9523941278457642\n"
     ]
    }
   ],
   "source": [
    "acc_tr, loss_tr = ensemble_model.evaluate_generator(train_data)\n",
    "print(acc_tr)\n",
    "print(loss_tr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "e74e3233",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\vigna\\AppData\\Local\\Temp\\ipykernel_26384\\3541006010.py:1: UserWarning: `Model.evaluate_generator` is deprecated and will be removed in a future version. Please use `Model.evaluate`, which supports generators.\n",
      "  acc_test, loss_test = ensemble_model.evaluate_generator(test_data)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.3334234058856964\n",
      "0.9085127115249634\n"
     ]
    }
   ],
   "source": [
    "\n",
    "acc_test, loss_test = ensemble_model.evaluate_generator(test_data)\n",
    "print(acc_test)\n",
    "print(loss_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bc4c679",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
